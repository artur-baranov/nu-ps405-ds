[
  {
    "objectID": "ps405-d_1.html",
    "href": "ps405-d_1.html",
    "title": "Last Quarter’s Review",
    "section": "",
    "text": "We are expected to have installed R and RStudio, if not see the installing R section.\nIn the discussion section, we will focus on coding and practicing what we have learned in the lectures.\nOffice hours are on Tuesday, 11-12:30 Scott 110.\nQuestions?\n\n\nDownload script"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PS405 Linear Models",
    "section": "",
    "text": "PS 405 • Winter 2025\n\nNorthwestern University\n\n\nThis site is designed to support students enrolled in the PS405 Linear Models course. Here, you will find lab materials, which are also available on Canvas. In the section, we will focus on coding and practicing the concepts covered in the lectures.\n\n\n\n\n\n   Thursday\n   11:00 AM – 11:50 AM\n   Scott Hall 212\n\n\n\n\n\n\n   Tuesday\n   11:00 AM – 12:30 PM\n   Scott Hall 110"
  },
  {
    "objectID": "ps405-d_1.html#terminology",
    "href": "ps405-d_1.html#terminology",
    "title": "Last Quarter’s Review",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "ps405-d_1.html#coding-terminology",
    "href": "ps405-d_1.html#coding-terminology",
    "title": "Last Quarter’s Review",
    "section": "Coding Terminology",
    "text": "Coding Terminology\n\nCode Chunk\nTo insert a Code Chunk, you can use Ctrl+Alt+I on Windows and Cmd+Option+I on Mac. Run the whole chunk by clicking the green triangle, or one/multiple lines by using Ctrl + Enter or Command + Return on Mac.\n\nprint(\"Code Chunk\")\n\n[1] \"Code Chunk\"\n\n\n\n\nFunction and Arguments\nMost of the functions we want to run require an argument For example, the function print() above takes the argument “Code Chunk”.\n\nfunction(argument)\n\n\n\nData structures\nThere are many data structures, but the most important to know the following.\n\nObjects. Those are individual units, e.g. a number or a word.\n\n\nnumber = 1\nnumber\n\nword = \"Northwestern\"\nword\n\n[1] 1\n[1] \"Northwestern\"\n\n\n\nVectors. Vectors are collections of objects. To create one, you will need to use function c().\n\n\nnumbers = c(1, 2, 3)\nnumbers\n\n[1] 1 2 3\n\n\n\nDataframes. Dataframes are the most used data structure. Last quarter you spend a lot of time working with it. It is a table with data. Columns are called variables, and those are vectors. You can access a column using $ operator.\n\n\ndf = data.frame(numbers, \n                numbers_multiplied = numbers * 2)\ndf\ndf$numbers_multiplied\n\n  numbers numbers_multiplied\n1       1                  2\n2       2                  4\n3       3                  6\n[1] 2 4 6\n\n\n\n\nData classes\nWe work with various classes of data, and the analysis we perform depends heavily on these classes.\n\nNumeric. Continuous data.\n\n\nnumeric_class = c(1.2, 2.5, 7.3)\nnumeric_class\nclass(numeric_class)\n\n[1] 1.2 2.5 7.3\n[1] \"numeric\"\n\n\n\nInteger. Whole numbers (e.g., count data).\n\n\ninteger_class = c(1:3)\nclass(integer_class)\n\n[1] \"integer\"\n\n\n\nCharacter. Usually, represent textual data.\n\n\nword\n\n[1] \"Northwestern\"\n\nclass(word)\n\n[1] \"character\"\n\n\n\nFactor. Categorical variables, where each value is treated as an identifier for a category.\n\n\ncolors = c(\"blue\", \"green\")\nclass(colors)\n\n[1] \"character\"\n\n\nAs you noticed, R did not identify the class of data correctly. We can change it using as.factor() function. You can easily change the class of your variable (as.numeric(), as.integer(), as.character())\n\ncolors = as.factor(colors)\nclass(colors)\n\n[1] \"factor\"\n\n\n\n\nLibraries\nQuite frequently, we use additional libraries to extend the capabilities of R. I’m sure you remember tidyverse. Let’s load it.\n\nlibrary(tidyverse)\n\nIf you updated your R or recently downloaded it, you can easily install libraries using the function install.packages().\n\n\nPipes\nPipes (%&gt;% or |&gt;) are helpful for streamlining the coding. They introduce linearity to the process of writing the code. In plain English, a pipe translates to “take an object, and then”.\n\nnumbers %&gt;%\n  print()\n\n[1] 1 2 3"
  },
  {
    "objectID": "ps405-d_2.html",
    "href": "ps405-d_2.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Download the data\nOrganize your directory\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_2.html#model-building",
    "href": "ps405-d_2.html#model-building",
    "title": "Simple Linear Regression",
    "section": "Model Building",
    "text": "Model Building\nLet’s run a simple model, and then check it’s summary.\n\nbasic_model = lm(Ladder_score ~ Social_support, whr)\n  \nsummary(basic_model)\n\n\nCall:\nlm(formula = Ladder_score ~ Social_support, data = whr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76562 -0.36701  0.01165  0.46577  1.49971 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.3428     0.3386  -1.013    0.313    \nSocial_support   7.3618     0.4183  17.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6304 on 135 degrees of freedom\nMultiple R-squared:  0.6964,    Adjusted R-squared:  0.6942 \nF-statistic: 309.7 on 1 and 135 DF,  p-value: &lt; 2.2e-16\n\n\nA one unit increase in Social Support is associated with a 7.4 increase in the happiness score. What is the maximum value the Happiness Score can take?\n\nmax(whr$Ladder_score)\n\n[1] 7.804\n\n\nAnd now, let’s draw a histogram of the Social Support. So, how much does this model tell us?\n\nggplot(whr) +\n  ...(aes(x = Social_support))\n\nLet’s correct the Social_support variable a bit, transforming it to 0-100 scale. What do you think about the model now? What do you think about \\(R^2\\)?\n\nwhr = whr %&gt;%\n  mutate(Social_support_percentage = Social_support * 100)\n\nadjusted_model = lm(Ladder_score ~ Social_support_percentage, whr)\n  \nsummary(adjusted_model)\n\n\nCall:\nlm(formula = Ladder_score ~ Social_support_percentage, data = whr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76562 -0.36701  0.01165  0.46577  1.49971 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -0.342811   0.338568  -1.013    0.313    \nSocial_support_percentage  0.073618   0.004183  17.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6304 on 135 degrees of freedom\nMultiple R-squared:  0.6964,    Adjusted R-squared:  0.6942 \nF-statistic: 309.7 on 1 and 135 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s write this regression formula out. Do you remember the general form?\n\\[\nY = \\beta_0 + \\beta_1X_1+\\epsilon\n\\]\nIn our case, this can be presented as\n\\[\n\\text{Happines} = -0.34 + 0.07\\text{ Social Support} + e\n\\]\nAlternatively,\n\\[\nY = -0.34+0.07x+u\n\\]\nNow, visualize the regression.\n\nggplot(whr, aes(x = Social_support_percentage, y = Ladder_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Social Support (%)\",\n       y = \"Happiness Score\")"
  },
  {
    "objectID": "ps405-d_2.html#diagnostics",
    "href": "ps405-d_2.html#diagnostics",
    "title": "Simple Linear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics\nLet’s analyze the regression. First, extract the residuals and plot their distribution. Does it follow \\(N(0, \\sigma^2)\\)?\n\nres = adjusted_model$residuals\n\nggplot() +\n  geom_histogram(aes(x = res), bins = 20) +\n  geom_vline(xintercept = mean(res), color = \"red\", size = 1.5)\n\n\n\n\n\n\n\n\nNow we need to check the constant variance assumption. Does it hold? What term is used to describe this satisfied assumption?\n\nyhat = adjusted_model$fitted.values\n\nggplot() +\n  geom_point(aes(x = yhat, y = res)) +\n  geom_hline(yintercept = 0, color = \"blue\") +\n  labs(title = \"Residuals vs fitted values plot\")\n\n\n\n\n\n\n\n\nExplore different patterns below"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "PS405 Linear Models",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nLink\n\n\nCode\n\n\nData\n\n\n\n\n\n\n1\n\n\nLast Quarter’s Review\n\n\n\n\n\n\n\n\n \n\n\n\n\n2\n\n\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\nReview and Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\nPresenting Results of Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\nPractice and Replication\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n\n\nRobust and Clustered Standard Errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\nModel Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\nGLMs and Quarter Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "PS405 Linear Models",
    "section": "",
    "text": "PS 405 • Winter 2025\n\nNorthwestern University\n\n\nThis site is designed to support students enrolled in the PS405 Linear Models course. Here, you will find lab materials, which are also available on Canvas. In the section, we will focus on coding and practicing the concepts covered in the lectures.\n\n\n\n\n\n   Thursday\n   11:00 AM – 11:50 AM\n   Scott Hall 212\n\n\n\n\n\n\n   Tuesday\n   11:00 AM – 12:30 PM\n   Scott Hall 110"
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "PS405 Linear Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHelpful Regression Resources\n\nStep-by-step regression guide\n\n\n\nData Wrangling\n\nJoins Tutorial\n\n\n\nData Sources\n\nPolitical Science datasets\n\n\n\nVisualization\n\nIntroduction to ggplot2\nMaps/Networks/Advanced visualization with ggplot2\nMaking interactive plots with ggplot2\n\n\n\nCheatsheets\n\nTidyverse cheatsheets"
  },
  {
    "objectID": "ps405-d_3.html",
    "href": "ps405-d_3.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Congrats with submitting the first HW! How are you feeling?\nThe discussion section structure (review, comments about HW, new material and exercises). I would be happy to hear your feedback after the classes or via email.\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_3.html#fixed-effects",
    "href": "ps405-d_3.html#fixed-effects",
    "title": "Multiple Linear Regression",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nLet’s explore how leader’s tenure is associated with the number of individuals in the government. We start with the simple linear regression. Take a moment to interpret the result and \\(R^2\\).\n\nlm(n_individuals ~ leaderexperience_continuous, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.666  -6.937  -1.937   5.301 109.063 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 23.59948    0.16232  145.39   &lt;2e-16 ***\nleaderexperience_continuous  0.33702    0.01627   20.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.21 on 9151 degrees of freedom\nMultiple R-squared:  0.04477,   Adjusted R-squared:  0.04467 \nF-statistic: 428.9 on 1 and 9151 DF,  p-value: &lt; 2.2e-16\n\n\nTake a moment and draw a scatterplot for n_individuals and leaderexperience_continuous. Add a regression line to the plot.\n\n...\n\nNow, let’s add a categorical variable, indep, to the model. By doing so, we assume that the association between the leader’s tenure and the number of individuals in the government differs depending on whether the leader is independent or partisan.\nPractically, this could be done in multiple ways. First, let’s discuss introduction of fixed effects to our model. Moreover, this is a Multiple linear regression!\n\nmodel_fe = lm(n_individuals ~ leaderexperience_continuous + indep, whogov) \nsummary(model_fe)\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous + indep, \n    data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.437  -7.078  -1.796   5.377 108.632 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 24.01096    0.16605  144.60   &lt;2e-16 ***\nleaderexperience_continuous  0.35700    0.01617   22.08   &lt;2e-16 ***\nindepIndependent            -3.02909    0.29988  -10.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 9126 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.05649,   Adjusted R-squared:  0.05628 \nF-statistic: 273.2 on 2 and 9126 DF,  p-value: &lt; 2.2e-16\n\n\nWe will use ggeffects library for visualization of regression with the fixed effects. This is sort of an addition to ggplot2 library from tidyverse. Don’t forget to install it using install.packages()!\n\nlibrary(ggeffects)\n\nThen, visualize the result. What can we see?\n\nggpredict(model_fe, terms = c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nLet’s customize the plot. It should be relatively straightforward given we know ggplot functions. Details for the customization of plot() function can be found on ggeffects website.\n\nggpredict(model_fe, terms= c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot(show_ci = F) +\n  labs(title = \"Fixed Effects Regression\",\n       x = \"Tenure of a Leader\",\n       y = \"Number of Individuals in a Cabinet\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSome common fixed effects include:\n\nCountry/Region/State\nIndividual leaders/Parties\nYear/Time\nPolicy presence or absence\n\nBy introducing fixed effects, we are able to control for unobserved confounders that vary across the units (not within!)."
  },
  {
    "objectID": "ps405-d_3.html#interactions",
    "href": "ps405-d_3.html#interactions",
    "title": "Multiple Linear Regression",
    "section": "Interactions",
    "text": "Interactions\nOften dummy variables are used to introduce an interaction term in the model. We will explore the association between Perceptions_of_corruption and number of people in the cabinet (n_individuals) depending on the independence of the party leader.\nThe task isn’t trivial as now we planning to use data from two datasets, Let’s subset those.\n\nwhr_subset = whr %&gt;%\n  select(Country_name, Perceptions_of_corruption)\n\nwhogov_subset = whogov %&gt;%\n  filter(year == 2021) %&gt;%\n  select(country_name, n_individuals, indep)\n\nNow, we are merging them. It’s not easy to understand what’s going on, but here is a great resource on joins (check “the {dplyr} way”)!\n\nwhr_whogov = whr_subset %&gt;%\n  left_join(whogov_subset, by = c(\"Country_name\" = \"country_name\")) \n\nCheck the result\n\nhead(whr_whogov)\n\n  Country_name Perceptions_of_corruption n_individuals           indep\n1      Finland                     0.182            23 Non-independent\n2      Denmark                     0.196            24 Non-independent\n3      Iceland                     0.668            15 Non-independent\n4       Israel                     0.708            33 Non-independent\n5  Netherlands                     0.379            18 Non-independent\n6       Sweden                     0.202            26 Non-independent\n\n\nNow, to interact variables we need to use asterisk *, i.e. multiplication.\n\nmodel_in = lm(Perceptions_of_corruption ~ n_individuals * indep, whr_whogov)\nsummary(model_in)\n\n\nCall:\nlm(formula = Perceptions_of_corruption ~ n_individuals * indep, \n    data = whr_whogov)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54000 -0.05737  0.04300  0.10864  0.24382 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     0.575192   0.061646   9.331 5.59e-16 ***\nn_individuals                   0.004818   0.002151   2.240   0.0269 *  \nindepIndependent                0.386220   0.159570   2.420   0.0170 *  \nn_individuals:indepIndependent -0.010703   0.005501  -1.946   0.0540 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1737 on 123 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.08324,   Adjusted R-squared:  0.06088 \nF-statistic: 3.723 on 3 and 123 DF,  p-value: 0.01326\n\n\nLet’s plot the result. Try to change show_ci to TRUE. Does it explain the p-value now?\n\nggpredict(model_in, terms= c(\"n_individuals\", \"indep\")) %&gt;%\n  plot(show_ci = FALSE) +\n  labs(title = \"Regression with Interaction Term\",\n       x = \"Number of Individuals in a Cabinet\",\n       y = \"Perception of Corruption\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAnd you can easily simulate the data (i.e., calculate the marginal effect) using ggpredict(). For example,\n\nggpredict(model_in, terms= c(\"n_individuals [12]\", \"indep [Independent]\"))\n\n# Predicted values of Perceptions_of_corruption\n\nn_individuals | Predicted |     95% CI\n--------------------------------------\n           12 |      0.89 | 0.71, 1.07"
  },
  {
    "objectID": "ps405-d_3.html#using-dummy-variables-in-the-regression",
    "href": "ps405-d_3.html#using-dummy-variables-in-the-regression",
    "title": "Multiple Linear Regression",
    "section": "Using dummy variables in the regression",
    "text": "Using dummy variables in the regression\nToday we are working with WhoGov dataset. As usual, I recomment taking a look at their codebook.\n\nwhogov = read.csv(\"data/WhoGov.csv\")\n\nFirst of all, these are the following variables we are going to work with today:\n\ncountry_name is a country name\nn_individuals number of unique persons in the cabinet\nleaderexperience_continuous the number of years the person has been leader of the country in total.\nleader_party party of the leader\nsystem_category the regime type\n\nStart with exploring the distribution of number of unique persons in the cabinet (n_individuals)\n\nggplot(whogov) +\n  geom_histogram(aes(x = n_individuals)) \n\n\n\n\n\n\n\n\nPresent the descriptive statistics of n_individuals variable.\n\n...\n\nLet’s explore whether a leader of a country being independent from a party leads to more or fewer people in their cabinet. First, let’s create a dummy variable indicating if a leader is independent or non-independent. You can use 1 or 0 instead, but to make it more readable we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Non-independent\"))\n\nNow, build a simple model and explore the effect.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           24.0592     0.2761  87.148   &lt;2e-16 ***\nindepNon-independent   2.2833     0.3058   7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 пропущенных наблюдений удалены)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14\n\n\nWhat if we want to relevel the indep variable to know the effect relative to Non-independent leader?\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nOops! This is why classes of data are important. Fix it!\n\nwhogov$indep = as.factor(whogov$indep)\n\nNow we can relevel the variable\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nCompare the models. Does the result sound reasonable? Pretty much. This is simply an inverse. But things get way more interesting if categorical variable has more than 2 levels. You will see this later on. For now, let’s interpret the result.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       26.3425     0.1315 200.264   &lt;2e-16 ***\nindepIndependent  -2.2833     0.3058  -7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 пропущенных наблюдений удалены)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14"
  },
  {
    "objectID": "ps405-d_3.html#dummy-variables-and-ols-regression",
    "href": "ps405-d_3.html#dummy-variables-and-ols-regression",
    "title": "Multiple Linear Regression",
    "section": "Dummy variables and OLS regression",
    "text": "Dummy variables and OLS regression\nToday we are working with WhoGov dataset. As usual, I recomment taking a look at their codebook.\n\nwhogov = read.csv(\"data/WhoGov.csv\")\n\nFirst of all, these are the following variables we are going to work with today:\n\ncountry_name is a country name\nn_individuals number of unique persons in the cabinet\nleaderexperience_continuous the number of years the person has been leader of the country in total.\nleader_party party of the leader\n\nStart with exploring the distribution of number of unique persons in the cabinet (n_individuals)\n\nggplot(whogov) +\n  geom_histogram(aes(x = n_individuals)) \n\n\n\n\n\n\n\n\nPresent the descriptive statistics of n_individuals variable.\n\n...\n\nLet’s examine whether a country’s leader being independent from a political party is associated with having more or fewer members in their cabinet. First, let’s create a dummy variable indicating if a leader is independent or non-independent. You can use 1 or 0 instead, but to make it more readable here we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Non-independent\"))\n\nNow, build a simple model and explore the effect. On average, being a non-independent (i.e. partisan) leader is associated with having 2.28 more members in their cabinet compared to independent leaders.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           24.0592     0.2761  87.148   &lt;2e-16 ***\nindepNon-independent   2.2833     0.3058   7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14\n\n\nWhat if we want to know the effect relative to Non-independent leader? Let’s relevel() the variable!\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nOops! This is why classes of data are important. Fix it!\n\nwhogov$indep = as.factor(whogov$indep)\n\nNow we can relevel the variable\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nCompare the models. Does the result sound reasonable? Pretty much. This is simply an inverse. But things get way more interesting if a categorical variable has more than 2 levels. You will see this later on.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       26.3425     0.1315 200.264   &lt;2e-16 ***\nindepIndependent  -2.2833     0.3058  -7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14"
  },
  {
    "objectID": "index.html#grade-increase-policy",
    "href": "index.html#grade-increase-policy",
    "title": "PS405 Linear Models",
    "section": "Grade increase policy",
    "text": "Grade increase policy\nTo improve your homework grade, you should submit the exercises from the corresponding lab session held during the week the homework was assigned. For instance, if you wish to improve the grade for the first homework, which was assigned on January 16th, you should submit the completed exercises from the second lab, which took place on the same day. If you prefer not to check the dates each time, given that homework is assigned each week starting from week 2, you can use the formula:\n\\[\n\\text{E}[\\text{Lab Number}|\\text{Pset is assigned each week}] = \\text{Pset Number} + 1\n\\]"
  },
  {
    "objectID": "index.html#pset-grade-increase-policy",
    "href": "index.html#pset-grade-increase-policy",
    "title": "PS405 Linear Models",
    "section": "Pset Grade Increase Policy",
    "text": "Pset Grade Increase Policy\nTo improve your grade for the problem set, you should submit the exercises from the corresponding lab session held during the week the homework was assigned. Exercises can be found at the end of each lab. You can increase grades for every problem set, except for the last one before the final. The maximum possible increase is two points.\nFor instance, if you wish to improve the grade for the first homework, which was assigned on January 16th, you should submit the completed exercises from the second lab, which took place on the same day. If you prefer not to check the dates each time, given that homework is assigned each week starting from week 2, you can use the formula:\n\\[\n\\text{Lab Number}|\\text{Pset is assigned each week} = \\text{Pset Number} + 1\n\\]\nAfter the grades are released, you have one week to submit the .qmd file via email to artur.baranov@u.northwestern.edu."
  },
  {
    "objectID": "ps405-d_4.html",
    "href": "ps405-d_4.html",
    "title": "Review and Confidence Intervals",
    "section": "",
    "text": "Congrats passing through the first quiz! How are you feeling?\nGrades for the psets are out.\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_4.html#latex-issues",
    "href": "ps405-d_4.html#latex-issues",
    "title": "Review and Confidence Intervals",
    "section": "\\(\\LaTeX\\) issues",
    "text": "\\(\\LaTeX\\) issues\nIntegration of R, Python, markdown, Latex and other useful languages incredibly useful. But it comes to a price that researchers should be careful. Any \\(\\LaTeX\\) code should go within two $ dollar signs $. For example, an inline formula looks like this: $ Y = 10 $, which produces the following result: \\(Y = 10\\). Alternatively, you can use a double dollar sign to start a “chunk” for latex. For example:\n\\[\nY = \\beta_0 + \\beta_1X + u\n\\]"
  },
  {
    "objectID": "ps405-d_4.html#useful-functions",
    "href": "ps405-d_4.html#useful-functions",
    "title": "Review and Confidence Intervals",
    "section": "Useful functions",
    "text": "Useful functions\nSometimes you might want to visualize some mathematical functions using geom_function(). In the HW, you were asked to plot an OLS regression vs the true data generating process. For this purpose example below is super useful (however, geom_abline() for this particular task is perfect, too).\n\nset.seed(123)\n\nexample_data = data.frame(x = rnorm(100,\n                                    mean = 2,\n                                    sd = 15),\n                          y = rnorm(100,\n                                    mean = 2,\n                                    sd = 15) * rnorm(100))\n\nggplot(example_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(aes(color = \"Fitted\"), method = \"lm\", se = F) +\n  geom_function(aes(color = \"Known\"), fun = function(x){1 + 0.5 * x}, size = 1.2) \n\n\n\n\n\n\n\n\nSometimes it’s useful to visualize, say, polynomials! How does \\(x^3\\) look like?\n\nggplot() +\n  geom_function(fun = function(x){x^2}) +\n  geom_vline(xintercept = 0) +\n  xlim(-1, 1)"
  },
  {
    "objectID": "ps405-d_4.html#fixed-effects",
    "href": "ps405-d_4.html#fixed-effects",
    "title": "Review and Confidence Intervals",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nWhen we introduce a dummy variable into the model, we quite often are interested in controlling for unobserved heterogeneity by allowing each category to have its own intercept. This is referred to as fixed effects.\nFirst, let’s create a dummy variable indicating if a leader is independent or Partisan. You can use 1 or 0 instead, but to make it more readable here we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Partisan\"))\n\nDon’t forget about the class of the variable!\n\nwhogov$indep = as.factor(whogov$indep)\n\nTake a moment to think about what unobserved heterogeneity we can control for by including whether a leader is independent or partisan in the model.\n\nmodel_fe = lm(n_individuals ~ leaderexperience_continuous + indep, whogov) \nsummary(model_fe)\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous + indep, \n    data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.437  -7.078  -1.796   5.377 108.632 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 20.98188    0.30295   69.26   &lt;2e-16 ***\nleaderexperience_continuous  0.35700    0.01617   22.08   &lt;2e-16 ***\nindepPartisan                3.02909    0.29988   10.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 9126 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.05649,   Adjusted R-squared:  0.05628 \nF-statistic: 273.2 on 2 and 9126 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s relevel to get the results for Independent candidates.\n\nwhogov$indep = relevel(whogov$indep, ref = \"Partisan\")\nmodel_fe = lm(n_individuals ~ leaderexperience_continuous + indep, whogov) \nsummary(model_fe)\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous + indep, \n    data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.437  -7.078  -1.796   5.377 108.632 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 24.01096    0.16605  144.60   &lt;2e-16 ***\nleaderexperience_continuous  0.35700    0.01617   22.08   &lt;2e-16 ***\nindepIndependent            -3.02909    0.29988  -10.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 9126 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.05649,   Adjusted R-squared:  0.05628 \nF-statistic: 273.2 on 2 and 9126 DF,  p-value: &lt; 2.2e-16\n\n\nTo understand what’s going on in the model we might want to visualize the result. Load the ggeffects library.\n\nlibrary(ggeffects)\n\nThen, visualize the result. What can we see?\n\nggpredict(model_fe, terms = c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot() +\n  labs(title = \"Fixed Effects Regression\",\n       x = \"Tenure of a Leader\",\n       y = \"Number of Individuals in a Cabinet\",\n       color = \"Leader's Status\") +\n  theme_bw()"
  },
  {
    "objectID": "ps405-d_4.html#interactions",
    "href": "ps405-d_4.html#interactions",
    "title": "Review and Confidence Intervals",
    "section": "Interactions",
    "text": "Interactions\nOften dummy variables are used to introduce an interaction term in the model. We will explore the association between Perceptions_of_corruption and number of people in the cabinet (n_individuals) depending on the independence of the party leader.\nThe task isn’t trivial as now we planning to use data from two datasets, Let’s subset those.\n\nwhr_subset = whr %&gt;%\n  select(Country_name, Perceptions_of_corruption)\n\nwhogov_subset = whogov %&gt;%\n  filter(year == 2021) %&gt;%\n  select(country_name, n_individuals, indep)\n\nNow, we are merging them. Once again, there is a great resource for joins (check “the {dplyr} way”)!\n\nwhr_whogov = whr_subset %&gt;%\n  left_join(whogov_subset, by = c(\"Country_name\" = \"country_name\")) \n\nCheck the result of the left_join()\n\nhead(whr_whogov)\n\n  Country_name Perceptions_of_corruption n_individuals    indep\n1      Finland                     0.182            23 Partisan\n2      Denmark                     0.196            24 Partisan\n3      Iceland                     0.668            15 Partisan\n4       Israel                     0.708            33 Partisan\n5  Netherlands                     0.379            18 Partisan\n6       Sweden                     0.202            26 Partisan\n\n\nNow, let’s interact the variable\n\nmodel_in = lm(Perceptions_of_corruption ~ n_individuals * indep, whr_whogov)\nsummary(model_in)\n\n\nCall:\nlm(formula = Perceptions_of_corruption ~ n_individuals * indep, \n    data = whr_whogov)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54000 -0.05737  0.04300  0.10864  0.24382 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     0.575192   0.061646   9.331 5.59e-16 ***\nn_individuals                   0.004818   0.002151   2.240   0.0269 *  \nindepIndependent                0.386220   0.159570   2.420   0.0170 *  \nn_individuals:indepIndependent -0.010703   0.005501  -1.946   0.0540 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1737 on 123 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.08324,   Adjusted R-squared:  0.06088 \nF-statistic: 3.723 on 3 and 123 DF,  p-value: 0.01326\n\n\nLet’s plot the result.\n\nggpredict(model_in, terms= c(\"n_individuals\", \"indep\")) %&gt;%\n  plot(show_ci = FALSE) +\n  labs(title = \"Regression with Interaction Term\",\n       x = \"Number of Individuals in a Cabinet\",\n       y = \"Perception of Corruption\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nI guess after solving pset and quiz you realize why interpreting interactions is hard? You can easily simulate the data (i.e., calculate the marginal effect) using ggpredict(). For example,\n\nggpredict(model_in, terms= c(\"n_individuals [12]\", \"indep [Independent]\"))\n\n# Predicted values of Perceptions_of_corruption\n\nn_individuals | Predicted |     95% CI\n--------------------------------------\n           12 |      0.89 | 0.71, 1.07"
  },
  {
    "objectID": "ps405-d_4.html#transformations",
    "href": "ps405-d_4.html#transformations",
    "title": "Confidence Intervals and Hypothesis Testing",
    "section": "Transformations",
    "text": "Transformations"
  },
  {
    "objectID": "ps405-d_4.html#iv-transformations",
    "href": "ps405-d_4.html#iv-transformations",
    "title": "Review and Confidence Intervals",
    "section": "IV Transformations",
    "text": "IV Transformations\nOne of the assumptions for OLS is that there is a linear relationship between the dependent and independent variables. However, this quite often not the case. Let’s reverse the correction made in World Happiness report data. See below. Does it look linear?\n\nggplot(whr) +\n  geom_point(aes(x = exp(Logged_GDP_per_capita), y = Ladder_score)) +\n  labs(x = \"GDP per capita\")\n\n\n\n\n\n\n\n\nIt doesn’t. It’s hard to describe this relationship in a linear manner. Natural log would explain this better, right?\n\nggplot(whr, aes(x = exp(Logged_GDP_per_capita), y = Ladder_score)) +\n  geom_point() +\n  geom_function(fun = function(x){log(x) - 4}) +\n  labs(x = \"GDP per capita\")\n\n\n\n\n\n\n\n\nThis is why we use the natural logarithm to transform GDP per capita. The transformation reveals a linear relationship between the two variables, allowing us to capture non-linear patterns in a linear format when using OLS regression. Another commonly used transformation is quadratic (\\(x^2\\)), which serves the same purpose of addressing non-linear relationships. We call latter ones polynomials.\n\nggplot(whr, aes(x = Logged_GDP_per_capita, y = Ladder_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "data_process.html",
    "href": "data_process.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nsipri = read.csv(\"import-export-values_1950-2024.csv\")\nsipri = sipri %&gt;%\n  select(-X2024)\n\n\nsipri = sipri %&gt;%\n  pivot_longer(2:75,\n               names_to = \"Year\",\n               values_to = \"Import\")\n\nsipri = sipri %&gt;%\n  mutate(Year = str_remove(Year, \"X\"))\n\nsipri = na.omit(sipri)\n\n\nload(url(\"https://github.com/vdeminstitute/vdemdata/raw/6bee8e170578fe8ccdc1414ae239c5e870996bc0/data/vdem.RData\"))\n\ne_v2x_polyarchy_5C\n\nvdem_sub = vdem %&gt;%\n  select(year, country_name, e_v2x_polyarchy_4C)\n\nvdem_sub$year = as.character(vdem_sub$year)\n\nmerger = sipri %&gt;%\n  left_join(vdem_sub, by = c('Year' = 'year', 'Recipient' = 'country_name'))\n\nmerger = na.omit(merger)\n\nmerger = merger %&gt;%\n  mutate(Regime = case_when(e_v2x_polyarchy_4C == 0 ~ \"Autocratic\",\n                            e_v2x_polyarchy_4C == 0.333 ~ \"Electoral Authoritarian\",\n                            e_v2x_polyarchy_4C == 0.667 ~ \"Minimally Democratic\",\n                            e_v2x_polyarchy_4C == 1 ~ \"Democratic\"))\n\nmerger = merger %&gt;%\n  select(-e_v2x_polyarchy_4C)\n\n# write.csv(merger, \"sipri.csv\", row.names = F)"
  },
  {
    "objectID": "ps405-d_5.html",
    "href": "ps405-d_5.html",
    "title": "Presenting Results of Regression",
    "section": "",
    "text": "Any questions?\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_5.html#tables",
    "href": "ps405-d_5.html#tables",
    "title": "Presenting Results of Regression",
    "section": "Tables",
    "text": "Tables\nThe most common way to present the resutls of the regression is in tables. We have practiced it over the last two quarters. By now, the expectation is you know how to present models in a “publishable” way. For details and examples see details.\nDon’t forget to:\n\nInclude p-values and \\(\\alpha\\) (critical values for stars display)!\nInclude standad errors or confidence intervals for \\(\\beta\\)\nIf you have a categorical variable, mention the reference category\n\\(R^2\\) for simple linear regression and \\(R^2_a\\) for multiple linear regression\nInclude number of observations\n\nAdd the following argument: output = \"table.html\".\n\nlibrary(modelsummary)\npublishable_table = modelsummary(list(\"Base model\" = model_basic,\n                    \"Fixed Effects model\" = model_fe,\n                    \"Interaction model\" = model_int),\n                   title = \"Arms Import Models\",  \n                   stars = TRUE,\n                   gof_omit = \"AIC|BIC|Log.Lik|F|RMSE\",\n                   coef_rename = c(\"(Intercept)\", \n                             \"Log GDP\", \n                             \"International Conflict\", \n                             \"Regime: Democratic\",\n                             \"Regime: Electoral Authoritarian\",\n                             \"Regime: Minimally Democratic\",\n                             \"Log GDP × International Conflict\"),\n                   notes = \"Regime reference category: Autocratic\")\n\npublishable_table\n\n\n\n    \n\n    \n    \n      \n        \n        Arms Import Models\n              \n                 \n                Base model\n                Fixed Effects model\n                Interaction model\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\nRegime reference category: Autocratic\n        \n                \n                  (Intercept)\n                  -718.034***\n                  -783.055***\n                  -747.804***\n                \n                \n                  \n                  (29.032)\n                  (36.852)\n                  (37.877)\n                \n                \n                  Log GDP\n                  108.547***\n                  126.364***\n                  122.263***\n                \n                \n                  \n                  (3.168)\n                  (4.468)\n                  (4.582)\n                \n                \n                  International Conflict\n                  \n                  206.923***\n                  -375.723*\n                \n                \n                  \n                  \n                  (26.470)\n                  (151.132)\n                \n                \n                  Regime: Democratic\n                  \n                  -95.328***\n                  -95.161***\n                \n                \n                  \n                  \n                  (17.191)\n                  (17.162)\n                \n                \n                  Regime: Electoral Authoritarian\n                  \n                  -151.434***\n                  -151.842***\n                \n                \n                  \n                  \n                  (23.328)\n                  (23.290)\n                \n                \n                  Regime: Minimally Democratic\n                  \n                  -216.183***\n                  -218.460***\n                \n                \n                  \n                  \n                  (50.820)\n                  (50.740)\n                \n                \n                  Log GDP × International Conflict\n                  \n                  \n                  63.002***\n                \n                \n                  \n                  \n                  \n                  (16.090)\n                \n                \n                  Num.Obs.\n                  6441\n                  4353\n                  4353\n                \n                \n                  R2\n                  0.154\n                  0.199\n                  0.202\n                \n                \n                  R2 Adj.\n                  0.154\n                  0.199\n                  0.201\n                \n        \n      \n    \n\n\n\nThe more complex the model, the harder it is to interpret the results. Let’s write out our models. You can often see these in articles. By plugging the coefficients we or our readers can calculate the predicted value for any GDP value, regime type and presence or absence of international conflict.\nThe basic model is easy, right?\n\\[\n\\text{Arms Import} = \\beta_0 + \\beta_1(\\text{LogGDP})\n\\] Now, fixed effects. It gets slightly trickier.\n\\[\n\\text{Arms Import} = \\beta_0 + \\beta_1(\\text{LogGDP}) + \\beta_2(\\text{International Conflict}) + \\beta_3(\\text{Regime})\n\\]\nAnd finally, interaction model. You can try to interpret the interaction, but we need to know the reference categories. Which is a bit of a hustle.\n\\[\n\\text{Arms Import} = \\beta_0 + \\beta_1(\\text{LogGDP}) + \\beta_2(\\text{International Conflict}) + \\beta_3(\\text{Regime}) + \\beta_4(\\text{LogGDP} \\times \\text{International Conflict})\n\\]"
  },
  {
    "objectID": "ps405-d_5.html#statistical-significante-and-magnitude-of-beta",
    "href": "ps405-d_5.html#statistical-significante-and-magnitude-of-beta",
    "title": "Presenting Results of Regression",
    "section": "Statistical Significante and Magnitude of \\(\\beta\\)",
    "text": "Statistical Significante and Magnitude of \\(\\beta\\)\nYou, probably, remember this from the problem sets. For yourself and reader it’s easier to compare effects visually.\nTo extract additional information automatically from the model, we use tidy() function from broom library. Take a moment to compare to “native” output.\n\nlibrary(broom)\nsummary(model_int)\n\n\nCall:\nlm(formula = Import ~ Log_GDP * International_conflict + Regime, \n    data = sipri_vdem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1183.9  -214.6   -83.1    77.6  5177.3 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -747.804     37.877 -19.743  &lt; 2e-16 ***\nLog_GDP                          122.263      4.582  26.682  &lt; 2e-16 ***\nInternational_conflict1         -375.723    151.132  -2.486    0.013 *  \nRegimeDemocratic                 -95.161     17.162  -5.545 3.12e-08 ***\nRegimeElectoral Authoritarian   -151.842     23.290  -6.520 7.85e-11 ***\nRegimeMinimally Democratic      -218.460     50.740  -4.305 1.70e-05 ***\nLog_GDP:International_conflict1   63.002     16.090   3.916 9.16e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 453.8 on 4346 degrees of freedom\n  (2524 observations deleted due to missingness)\nMultiple R-squared:  0.2023,    Adjusted R-squared:  0.2012 \nF-statistic: 183.7 on 6 and 4346 DF,  p-value: &lt; 2.2e-16\n\n\nThis pretty much the dataframe with the regression output. Thus, it simplifies our work for visualization purposes! (and similar functionality is available in modelsummary library with get_estimates() command)\n\nmodel_int_output = tidy(model_int, conf.int = TRUE, conf.level = 0.95)\nmodel_int_output\n\n# A tibble: 7 × 7\n  term                 estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)            -748.      37.9     -19.7  3.69e- 83   -822.     -674. \n2 Log_GDP                 122.       4.58     26.7  2.19e-145    113.      131. \n3 International_confl…   -376.     151.       -2.49 1.30e-  2   -672.      -79.4\n4 RegimeDemocratic        -95.2     17.2      -5.54 3.12e-  8   -129.      -61.5\n5 RegimeElectoral Aut…   -152.      23.3      -6.52 7.85e- 11   -198.     -106. \n6 RegimeMinimally Dem…   -218.      50.7      -4.31 1.70e-  5   -318.     -119. \n7 Log_GDP:Internation…     63.0     16.1       3.92 9.16e-  5     31.5      94.5\n\n\nPlotting the effects is great to provide intuition about the significance and the magnitude of the effects. But! Don’t forget about the scales! Are you comparing comparable? Scaling may be helpful: how can you compare one unit increase in Log_GDP to “one unit increase” in International_conflict?\nMoreover, you cannot interpret the effect of International_conflict or Log_GDP without accounting for the interaction term. In our case, this plot is not publishible and is quite misleading. But in the exercises you’ll create a publishable plot for the fixed effects model.\n\nggplot(model_int_output) +\n  geom_linerange(aes(x = term,\n                    y = estimate,\n                    ymin = conf.low,\n                    ymax = conf.high)) +\n  geom_point(aes(x = term,\n                 y = estimate)) +\n  geom_hline(yintercept = 0, lty=2, color=\"gray\") +\n  coord_flip()"
  },
  {
    "objectID": "ps405-d_5.html#report-predicted-values",
    "href": "ps405-d_5.html#report-predicted-values",
    "title": "Presenting Results of Regression",
    "section": "Report predicted values",
    "text": "Report predicted values\nFor example, we take the most typical GDP, and calculate the effects for two categorical variables: International_conflict and Regime.\n\nggpredict(model_int, terms = c(\"International_conflict\", \"Regime\")) \n\n# Predicted values of Import\n\nRegime: Autocratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    308.57 | 288.87, 328.27\n1                      |    477.20 | 423.80, 530.59\n\nRegime: Democratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    213.41 | 187.75, 239.07\n1                      |    382.03 | 322.22, 441.85\n\nRegime: Electoral Authoritarian\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    156.73 | 114.75, 198.70\n1                      |    325.35 | 258.22, 392.48\n\nRegime: Minimally Democratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |     90.11 |  -7.60, 187.82\n1                      |    258.74 | 146.76, 370.71\n\nAdjusted for:\n* Log_GDP = 8.64\n\n\nLet’s make it look publishable. We wrangle the data a bit. By now, the code shouldn’t stress you out.\n\nmodel_int_groupped = ggpredict(model_int, terms = c(\"International_conflict\", \"Regime\")) %&gt;%\n  as.data.frame() %&gt;%\n  rename(International_conflict = x,\n         Regime = group,\n         Import = predicted) \n\nmodel_int_groupped\n\n  International_conflict    Import std.error   conf.low conf.high\n1                      0 308.56877  10.04954 288.866542  328.2710\n2                      0 213.40769  13.08769 187.749148  239.0662\n3                      0 156.72724  21.40898 114.754718  198.6998\n4                      0  90.10883  49.84073  -7.604411  187.8221\n5                      1 477.19551  27.23333 423.804293  530.5867\n6                      1 382.03443  30.51067 322.217952  441.8509\n7                      1 325.35398  34.24112 258.223927  392.4840\n8                      1 258.73557  57.11487 146.761296  370.7098\n                   Regime\n1              Autocratic\n2              Democratic\n3 Electoral Authoritarian\n4    Minimally Democratic\n5              Autocratic\n6              Democratic\n7 Electoral Authoritarian\n8    Minimally Democratic\n\n\nFinally, present the marginal effects using tinytable.1 The code below is quite messy, in your free time you can explore it in a more detail. For now, I want this code to be available to you. Add this line of code in the end: save_tt(\"predicted_values.html\")\n\nlibrary(tinytable)\n\nmodel_int_groupped %&gt;%\n  group_by(International_conflict, Regime) %&gt;%\n  summarize(Import = paste0(round(Import, 3), \"\\n\", \"[\", round(conf.low, 3), \"; \", round(conf.high, 3), \"]\")) %&gt;%\n  pivot_wider(names_from = Regime,\n              values_from = Import) %&gt;%\n  mutate(International_conflict = ifelse(International_conflict == 1, \"Present\", \"Absent\")) %&gt;%\n  rename(`International Conflict` = International_conflict) %&gt;% \n  tt(note = \"Adjusted for average log of GDP (=8.64)\") %&gt;%\n  group_tt(j = list(\"Regime\" = 2:5)) \n\n`summarise()` has grouped output by 'International_conflict'. You can override\nusing the `.groups` argument.\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nRegime\n\n        \n              \n                International Conflict\n                Autocratic\n                Democratic\n                Electoral Authoritarian\n                Minimally Democratic\n              \n        \n        Adjusted for average log of GDP (=8.64)\n        \n                \n                  Absent \n                  308.569\n[288.867; 328.271]\n                  213.408\n[187.749; 239.066]\n                  156.727\n[114.755; 198.7]  \n                  90.109\n[-7.604; 187.822] \n                \n                \n                  Present\n                  477.196\n[423.804; 530.587]\n                  382.034\n[322.218; 441.851]\n                  325.354\n[258.224; 392.484]\n                  258.736\n[146.761; 370.71]\n                \n        \n      \n    \n\n\n\nWhy voters who value democracy participate in democratic backsliding: 8"
  },
  {
    "objectID": "ps405-d_5.html#footnotes",
    "href": "ps405-d_5.html#footnotes",
    "title": "Presenting Results of Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGreat example of research presenting the results in a similar manner can be found in Reuter, O.J. and Szakonyi, D., 2015. Online social media and political awareness in authoritarian regimes. British Journal of Political Science, 45(1), pp.29-51.↩︎"
  },
  {
    "objectID": "ps405-d_5.html#statistical-significance-and-magnitude-of-beta",
    "href": "ps405-d_5.html#statistical-significance-and-magnitude-of-beta",
    "title": "Presenting Results of Regression",
    "section": "Statistical Significance and Magnitude of \\(\\beta\\)",
    "text": "Statistical Significance and Magnitude of \\(\\beta\\)\nYou, probably, remember this from the problem sets. For yourself and reader it’s easier to compare effects visually.\nTo extract additional information automatically from the model, we use tidy() function from broom library. Take a moment to compare to “native” output.\n\nlibrary(broom)\nsummary(model_int)\n\n\nCall:\nlm(formula = Import ~ Log_GDP * International_conflict + Regime, \n    data = sipri_vdem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1183.9  -214.6   -83.1    77.6  5177.3 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     -747.804     37.877 -19.743  &lt; 2e-16 ***\nLog_GDP                          122.263      4.582  26.682  &lt; 2e-16 ***\nInternational_conflict1         -375.723    151.132  -2.486    0.013 *  \nRegimeDemocratic                 -95.161     17.162  -5.545 3.12e-08 ***\nRegimeElectoral Authoritarian   -151.842     23.290  -6.520 7.85e-11 ***\nRegimeMinimally Democratic      -218.460     50.740  -4.305 1.70e-05 ***\nLog_GDP:International_conflict1   63.002     16.090   3.916 9.16e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 453.8 on 4346 degrees of freedom\n  (2524 observations deleted due to missingness)\nMultiple R-squared:  0.2023,    Adjusted R-squared:  0.2012 \nF-statistic: 183.7 on 6 and 4346 DF,  p-value: &lt; 2.2e-16\n\n\nThis pretty much the dataframe with the regression output. Thus, it simplifies our work for visualization purposes! (and similar functionality is available in modelsummary library with get_estimates() command)\n\nmodel_int_output = tidy(model_int, conf.int = TRUE, conf.level = 0.95)\nmodel_int_output\n\n# A tibble: 7 × 7\n  term                 estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)            -748.      37.9     -19.7  3.69e- 83   -822.     -674. \n2 Log_GDP                 122.       4.58     26.7  2.19e-145    113.      131. \n3 International_confl…   -376.     151.       -2.49 1.30e-  2   -672.      -79.4\n4 RegimeDemocratic        -95.2     17.2      -5.54 3.12e-  8   -129.      -61.5\n5 RegimeElectoral Aut…   -152.      23.3      -6.52 7.85e- 11   -198.     -106. \n6 RegimeMinimally Dem…   -218.      50.7      -4.31 1.70e-  5   -318.     -119. \n7 Log_GDP:Internation…     63.0     16.1       3.92 9.16e-  5     31.5      94.5\n\n\nPlotting the effects is great to provide intuition about the significance and the magnitude of the effects. But! Don’t forget about the scales! Are you comparing comparable? Scaling may be helpful: how can you compare one unit increase in Log_GDP to “one unit increase” in International_conflict?\nMoreover, you cannot interpret the effect of International_conflict or Log_GDP without accounting for the interaction term. In our case, this plot is not publishible and is quite misleading. It is more suitable for fixed effects and simple additive models. However, don’t forget about the scale!\n\nggplot(model_int_output) +\n  geom_linerange(aes(x = term,\n                    y = estimate,\n                    ymin = conf.low,\n                    ymax = conf.high)) +\n  geom_point(aes(x = term,\n                 y = estimate)) +\n  geom_hline(yintercept = 0, lty=2, color=\"gray\") +\n  coord_flip()"
  },
  {
    "objectID": "ps405-d_5.html#predicted-values",
    "href": "ps405-d_5.html#predicted-values",
    "title": "Presenting Results of Regression",
    "section": "Predicted values",
    "text": "Predicted values\nPresenting predicted values provides the reader with intuition about what happens when variables take specific values. For example, we take the most typical GDP, and calculate the effects for two categorical variables: International_conflict and Regime.\n\nggpredict(model_int, terms = c(\"International_conflict\", \"Regime\")) \n\n# Predicted values of Import\n\nRegime: Autocratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    308.57 | 288.87, 328.27\n1                      |    477.20 | 423.80, 530.59\n\nRegime: Democratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    213.41 | 187.75, 239.07\n1                      |    382.03 | 322.22, 441.85\n\nRegime: Electoral Authoritarian\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |    156.73 | 114.75, 198.70\n1                      |    325.35 | 258.22, 392.48\n\nRegime: Minimally Democratic\n\nInternational_conflict | Predicted |         95% CI\n---------------------------------------------------\n0                      |     90.11 |  -7.60, 187.82\n1                      |    258.74 | 146.76, 370.71\n\nAdjusted for:\n* Log_GDP = 8.64\n\n\nLet’s make it look publishable. We wrangle the data a bit. By now, the code shouldn’t stress you out. If it does, then run it step-by-step to get a sense of what’s going on!\n\nmodel_int_groupped = ggpredict(model_int, terms = c(\"International_conflict\", \"Regime\")) %&gt;%\n  as.data.frame() %&gt;%\n  rename(International_conflict = x,\n         Regime = group,\n         Import = predicted) \n\nmodel_int_groupped\n\n  International_conflict    Import std.error   conf.low conf.high\n1                      0 308.56877  10.04954 288.866542  328.2710\n2                      0 213.40769  13.08769 187.749148  239.0662\n3                      0 156.72724  21.40898 114.754718  198.6998\n4                      0  90.10883  49.84073  -7.604411  187.8221\n5                      1 477.19551  27.23333 423.804293  530.5867\n6                      1 382.03443  30.51067 322.217952  441.8509\n7                      1 325.35398  34.24112 258.223927  392.4840\n8                      1 258.73557  57.11487 146.761296  370.7098\n                   Regime\n1              Autocratic\n2              Democratic\n3 Electoral Authoritarian\n4    Minimally Democratic\n5              Autocratic\n6              Democratic\n7 Electoral Authoritarian\n8    Minimally Democratic\n\n\nFinally, present the marginal effects using tinytable.1 The code below is quite messy, in your free time you can explore it in a more detail. For now, I want this code to be available to you. Add this line of code in the end: save_tt(\"predicted_values.html\") to save the table on your machine.\n\nlibrary(tinytable)\n\nmodel_int_groupped %&gt;%\n  group_by(International_conflict, Regime) %&gt;%\n  summarize(Import = paste0(round(Import, 3), \"\\n\", \"[\", round(conf.low, 3), \"; \", round(conf.high, 3), \"]\")) %&gt;%\n  pivot_wider(names_from = Regime,\n              values_from = Import) %&gt;%\n  mutate(International_conflict = ifelse(International_conflict == 1, \"Present\", \"Absent\")) %&gt;%\n  rename(`International Conflict` = International_conflict) %&gt;% \n  tt(note = \"Adjusted for average log of GDP (=8.64)\") %&gt;%\n  group_tt(j = list(\"Regime\" = 2:5)) \n\n\n\n    \n\n    \n    \n      \n        \n\n \nRegime\n\n        \n              \n                International Conflict\n                Autocratic\n                Democratic\n                Electoral Authoritarian\n                Minimally Democratic\n              \n        \n        Adjusted for average log of GDP (=8.64)\n        \n                \n                  Absent\n                  308.569\n[288.867; 328.271]\n                  213.408\n[187.749; 239.066]\n                  156.727\n[114.755; 198.7]\n                  90.109\n[-7.604; 187.822]\n                \n                \n                  Present\n                  477.196\n[423.804; 530.587]\n                  382.034\n[322.218; 441.851]\n                  325.354\n[258.224; 392.484]\n                  258.736\n[146.761; 370.71]"
  },
  {
    "objectID": "ps405-d_5.html#reporting-marginal-effects",
    "href": "ps405-d_5.html#reporting-marginal-effects",
    "title": "Presenting Results of Regression",
    "section": "Reporting Marginal Effects",
    "text": "Reporting Marginal Effects\nCalculate the most “typical” observation (or simply the average observation) across all indepdenent variables.\n\ntypical_obs = sipri_vdem %&gt;%\n      summarize(Log_GDP = mean(Log_GDP, na.rm = T),\n                International_conflict = table(International_conflict) %&gt;% which.max() %&gt;% names(),\n                Regime = table(Regime) %&gt;% which.max() %&gt;% names())\n\ntypical_obs\n\n   Log_GDP International_conflict     Regime\n1 8.984952                      0 Autocratic\n\n\nThis would be our baseline.\n\nggpredict(model_int, typical_obs)\n\n# Predicted values of Import\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.98 |    350.72 | 329.90, 371.54\n\n\nAnd now you can easily calculate the average marginal effect with one unit increase in some X.\n\nmarginal_effects = list(Log_GDP = c(8.984952, 8, 8.5, 9.5, 10),\n                        International_conflict = c(\"0\"),\n                        Regime = c(\"Autocratic\", \"Democratic\"))\n\n\nggpredict(model_int, marginal_effects) \n\n# Predicted values of Import\n\nRegime: Autocratic\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.00 |    230.30 | 211.46, 249.13\n   8.50 |    291.43 | 272.06, 310.80\n   8.98 |    350.72 | 329.90, 371.54\n   9.50 |    413.69 | 390.51, 436.87\n  10.00 |    474.82 | 448.77, 500.88\n\nRegime: Democratic\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.00 |    135.14 | 106.89, 163.38\n   8.50 |    196.27 | 170.13, 222.41\n   8.98 |    255.56 | 230.85, 280.27\n   9.50 |    318.53 | 294.54, 342.51\n  10.00 |    379.66 | 355.54, 403.78"
  },
  {
    "objectID": "ps405-d_5.html#marginal-effects",
    "href": "ps405-d_5.html#marginal-effects",
    "title": "Presenting Results of Regression",
    "section": "Marginal Effects",
    "text": "Marginal Effects\nCalculate the “typical” observation (or simply the average observation) across all independent variables.\n\ntypical_obs = sipri_vdem %&gt;%\n      summarize(Log_GDP = mean(Log_GDP, na.rm = T),\n                International_conflict = table(International_conflict) %&gt;% which.max() %&gt;% names(),\n                Regime = table(Regime) %&gt;% which.max() %&gt;% names())\n\ntypical_obs\n\n   Log_GDP International_conflict     Regime\n1 8.984952                      0 Autocratic\n\n\nThis would be our baseline. We can calculate the predicted value for the typical observation.\n\nggpredict(model_int, typical_obs)\n\n# Predicted values of Import\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.98 |    350.72 | 329.90, 371.54\n\n\nHowever, the most interesting is to know how the DV change if the IV changes (holding everything else constant). Basically, we are trying to get a simple interpretation, as it is with the simple additive OLS. You can easily calculate the average marginal effect with one unit change in some X with ggeffects library. Moreover, you can illustrate how the predicted values change in comparison to the most typical observation.\n\nmarginal_effects = list(Log_GDP = c(8.984952, 8, 8.5, 9.5, 10),\n                        International_conflict = c(\"0\"),\n                        Regime = c(\"Autocratic\", \"Democratic\"))\n\n\nggpredict(model_int, marginal_effects) \n\n# Predicted values of Import\n\nRegime: Autocratic\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.00 |    230.30 | 211.46, 249.13\n   8.50 |    291.43 | 272.06, 310.80\n   8.98 |    350.72 | 329.90, 371.54\n   9.50 |    413.69 | 390.51, 436.87\n  10.00 |    474.82 | 448.77, 500.88\n\nRegime: Democratic\n\nLog_GDP | Predicted |         95% CI\n------------------------------------\n   8.00 |    135.14 | 106.89, 163.38\n   8.50 |    196.27 | 170.13, 222.41\n   8.98 |    255.56 | 230.85, 280.27\n   9.50 |    318.53 | 294.54, 342.51\n  10.00 |    379.66 | 355.54, 403.78"
  },
  {
    "objectID": "ds6_drafts.html",
    "href": "ds6_drafts.html",
    "title": "Untitled",
    "section": "",
    "text": "library(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\neubarometer = read_dta(\"data/eurobarometer100_2.dta\")\n\nd70 - LIFE SATISFACTION qa1_1 - SITUATION: COUNTRY GENERAL qa3_1 - IMPORTANT ISSUES CNTRY: CRIME\n\nsub = eubarometer %&gt;%\n  select(d70, qa1_1, qa3_1, qa5_15, d73_3)\n\n# sub$qa1_1 %&gt;% hist()\n\nmodel = lm(d70 ~ ., sub)\nsummary(model)\n\n\nCall:\nlm(formula = d70 ~ ., data = sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7352 -0.4336 -0.0936  0.2057  3.2064 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.241309   0.014359  86.447  &lt; 2e-16 ***\nqa1_1        0.296907   0.004530  65.546  &lt; 2e-16 ***\nqa3_1       -0.003106   0.012514  -0.248    0.804    \nqa5_15      -0.043104   0.007928  -5.437 5.46e-08 ***\nd73_3        0.002331   0.003751   0.621    0.534    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.702 on 35570 degrees of freedom\nMultiple R-squared:  0.111, Adjusted R-squared:  0.1109 \nF-statistic:  1110 on 4 and 35570 DF,  p-value: &lt; 2.2e-16\n\nplot(model, 1)"
  },
  {
    "objectID": "ps405-d_6.html",
    "href": "ps405-d_6.html",
    "title": "Practice and Replication",
    "section": "",
    "text": "Any questions?\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_7.html",
    "href": "ps405-d_7.html",
    "title": "Robust and Clustered Standard Errors",
    "section": "",
    "text": "Before we start\n\nAny questions?\n\n\nDownload script\n\n\n\nDownload data\n\n\n\nReview of the previous week\n\n\n\n\n\n\nReview\n\n\n\nLoad tidyverse library\n\nlibrary(tidyverse)\n\nLoad the George Ward’s replication data\nusing ggpairs() from GGally library, visualize the relationships between the variables. What can you see?\nExplore the relationship between subjective well being (satislfe_survey_mean) and number of parties in government (parties_ingov). Draw a boxplot. Does it look right? If not, correct!\nSet up a pooled model. Predict satislfe_survey_mean by parties_ingov. Save the model to model_pooled object. Present summary. What do you think about the model?\nIntroduce country fixed effects. Save it to the model_countryfe object. Present summary. Compare it to the pooled model. Pay attention to \\(R^2\\). Why are they different?\nUsing ggpredict() from ggeffects library visualize model_countryfe. Plot parties_ingov and the following country values: BEL, DNK, SWE. Take a moment to understand the graph. Insert the argument connect_line = TRUE to make the comprehension of the graph easier.\n\n\n\n\nReview of the homework\nThe set.seed() function allows you to make the random process “controllable”. To get the sense of what’s going on, let’s experiment a bit.\nLet’s randomly generate 10 integers. Each time you run the chunk below, a different vector is generated.\n\nsample.int(100, 10)\n\n [1] 78 29 62  2 50 80 69 34 65 93\n\n\nNow, if we want to get the same results each time, we can set the seed.\n\nset.seed(123)\n\nsample.int(100, 10)\n\n [1] 31 79 51 14 67 42 50 43 97 25\n\n\nBe careful when you include the set.seed() within functions.\n\nwrongfunc = function(x){\n  set.seed(123)\n\n  sample.int(x, 10)\n}\n\nIn simulations it wouldn’t allow you to randomly sample/generate data.\n\nfor(i in 1:10){\n  print(wrongfunc(100))\n}\n\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 31 79 51 14 67 42 50 43 97 25\n\n\nInstead, you would want that the loop would produce the constant result.\n\ncorrfunc = function(x){\n  sample.int(x, 10)\n}\n\nset.seed(123)\nfor(i in 1:10){\n  print(corrfunc(100))\n}\n\n [1] 31 79 51 14 67 42 50 43 97 25\n [1] 90 91 69 99 57 92  9 93 72 26\n [1]  7 42  9 83 36 78 81 43 76 15\n [1] 32  7  9 41 74 23 27 60 53 99\n [1] 53 27 96 38 89 34 93 69 72 76\n [1] 63 13 82 97 91 25 38 21 79 41\n [1] 47 90 60 95 16 94  6 72 86 92\n [1] 39 31 81 50 34  4 13 69 25 52\n [1] 22 89 32 25 87 35 40 30 12 31\n [1] 30 64 14 93 96 71 67 23 79 85\n\n\n\n\nAgenda\n\nIntroduction to diagnostics\nFixing heteroscedasticity\nAnd if we can’t, then we will be working with robust standard errors\nFinally, dealing with clustered standard errors\n\n\n\nCountry-Year Fixed Effects Model\nLet’s explore Comparative Political Dataset. It consists of political and institutional country-level data. Take a look on their codebook.\nToday we are working with the following variables.\n\nprefisc_gini - Gini index. What is it?\nopenc - Openness of the economy (trade as % of GDP)\nservadmi_pmp - Public and mandatory private employment services and administration as a percentage of GDP.\ncountry and year\n\nFirst of all, let’s load the data\n\nlibrary(readxl)\ncpds = read_excel(\"data/cpds.xlsx\")\n\nImagine you are interested in explaining inequality (prefisc_gini) by the amount of trade (measured as oppenness of the economy – openc). Set up a simple linear regression (slr).\n\nmodel_slr = lm(prefisc_gini ~ openc, cpds)\nsummary(model_slr)\n\n\nCall:\nlm(formula = prefisc_gini ~ openc, data = cpds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7307  -2.8912   0.5475   2.8113  12.9869 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.457621   0.295214 140.432   &lt;2e-16 ***\nopenc       -0.001798   0.002652  -0.678    0.498    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.141 on 547 degrees of freedom\n  (1317 observations deleted due to missingness)\nMultiple R-squared:  0.00084,   Adjusted R-squared:  -0.0009866 \nF-statistic: 0.4599 on 1 and 547 DF,  p-value: 0.498\n\n\nGiven how complex the inequality, it’s fair to assume we have some confounders. Let’s control for labour market policies (measured by Public employment as % of GDP – servadmi_pmp). Set up a multiple linear regression (MLR) below. What do you think about the model?\n\nmodel_mlr = lm(prefisc_gini ~ openc + servadmi_pmp, cpds)\nsummary(model_mlr)\n\n\nCall:\nlm(formula = prefisc_gini ~ openc + servadmi_pmp, data = cpds)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.2821  -2.5147   0.3219   2.6891  13.0194 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  42.216729   0.454877  92.809  &lt; 2e-16 ***\nopenc        -0.007419   0.002691  -2.758  0.00605 ** \nservadmi_pmp  1.717317   1.862425   0.922  0.35695    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.934 on 477 degrees of freedom\n  (1386 observations deleted due to missingness)\nMultiple R-squared:  0.01985,   Adjusted R-squared:  0.01574 \nF-statistic: 4.829 on 2 and 477 DF,  p-value: 0.008388\n\n\nHow can we check if the homoscedasticity assumption is satisfied? Proceed with simple analysis. Extract residuals and fitted (predicted) values. Plot it. What do you think?\n\nres = model_mlr$residuals\nfit_val = model_mlr$fitted.values\n\nggplot() +\n  geom_point(aes(x = fit_val, y = res)) +\n  geom_hline(yintercept = 0) \n\n\n\n\n\n\n\n\nThe same plot can be easily accessed using the base R functions. We’ll get to this next week in a more detail.\n\nplot(model_mlr, which = 1)\n\n\n\n\n\n\n\n\nWe know that our data is of country-year structure. Let’s introduce the fixed effects to the model. First, make sure these are factors.\n\ncpds$country = as.factor(cpds$country)\ncpds$year = as.factor(cpds$year)\n\nWhat has changed?\n\nmodel_fe = lm(prefisc_gini ~ openc + servadmi_pmp + country + year, cpds)\nsummary(model_fe)\n\n\nCall:\nlm(formula = prefisc_gini ~ openc + servadmi_pmp + country + \n    year, data = cpds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4138 -1.1115 -0.0014  0.9614  6.5810 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            35.437237   1.560752  22.705  &lt; 2e-16 ***\nopenc                   0.011568   0.004632   2.497 0.012898 *  \nservadmi_pmp            2.446305   1.717145   1.425 0.155022    \ncountryAustria         -3.621612   0.771798  -4.692 3.68e-06 ***\ncountryBelgium         -3.636327   0.879871  -4.133 4.35e-05 ***\ncountryCanada          -1.604814   0.711066  -2.257 0.024538 *  \ncountryCzech Republic  -6.077881   1.015740  -5.984 4.75e-09 ***\ncountryDenmark         -4.564718   0.887802  -5.142 4.22e-07 ***\ncountryEstonia         -4.319239   1.184771  -3.646 0.000301 ***\ncountryFinland         -2.176054   0.923626  -2.356 0.018942 *  \ncountryFrance           0.007526   0.746337   0.010 0.991959    \ncountryGermany         -2.894690   0.735793  -3.934 9.80e-05 ***\ncountryGreece          -0.290260   1.005907  -0.289 0.773067    \ncountryHungary          1.280997   1.088680   1.177 0.240016    \ncountryIceland        -11.037538   1.339412  -8.241 2.32e-15 ***\ncountryIreland          2.232621   0.956071   2.335 0.020013 *  \ncountryItaly           -1.154673   0.991811  -1.164 0.245016    \ncountryJapan           -7.228360   1.331619  -5.428 9.75e-08 ***\ncountryLuxembourg      -6.698123   1.312531  -5.103 5.11e-07 ***\ncountryNetherlands     -3.895918   0.951399  -4.095 5.09e-05 ***\ncountryNorway          -5.508208   0.901830  -6.108 2.34e-09 ***\ncountryPoland          -0.011346   0.801278  -0.014 0.988709    \ncountryRomania         -2.871224   0.836577  -3.432 0.000660 ***\ncountrySlovakia        -9.720522   1.029048  -9.446  &lt; 2e-16 ***\ncountrySlovenia        -8.015692   1.168882  -6.858 2.58e-11 ***\ncountrySpain           -0.276633   0.735491  -0.376 0.707022    \ncountrySweden          -5.162188   0.776541  -6.648 9.49e-11 ***\ncountrySwitzerland    -10.486322   0.834016 -12.573  &lt; 2e-16 ***\ncountryUnited Kingdom   1.696744   0.724065   2.343 0.019587 *  \ncountryUSA              2.433579   0.735544   3.309 0.001020 ** \nyear1981               -1.544550   2.429892  -0.636 0.525361    \nyear1982                1.455276   2.429893   0.599 0.549566    \nyear1983                6.820538   2.008574   3.396 0.000751 ***\nyear1984                1.041995   2.430511   0.429 0.668356    \nyear1985                3.685598   1.599656   2.304 0.021721 *  \nyear1986                1.800695   1.637568   1.100 0.272143    \nyear1987                4.646000   1.556861   2.984 0.003013 ** \nyear1988                3.458806   1.634901   2.116 0.034979 *  \nyear1989                3.340307   1.635580   2.042 0.041762 *  \nyear1990                3.675834   1.579358   2.327 0.020428 *  \nyear1991                3.600641   1.610814   2.235 0.025935 *  \nyear1992                5.821592   1.534969   3.793 0.000171 ***\nyear1993                6.358422   1.605394   3.961 8.81e-05 ***\nyear1994                7.223842   1.568944   4.604 5.53e-06 ***\nyear1995                7.112686   1.506798   4.720 3.23e-06 ***\nyear1996                7.593346   1.540853   4.928 1.21e-06 ***\nyear1997                7.216507   1.560473   4.625 5.04e-06 ***\nyear1998                7.325058   1.562397   4.688 3.75e-06 ***\nyear1999                6.804528   1.540786   4.416 1.29e-05 ***\nyear2000                6.705205   1.502256   4.463 1.04e-05 ***\nyear2001                7.792546   1.609835   4.841 1.84e-06 ***\nyear2002                7.324104   1.556131   4.707 3.45e-06 ***\nyear2003                7.466408   1.546082   4.829 1.94e-06 ***\nyear2004                8.118553   1.482882   5.475 7.64e-08 ***\nyear2005                8.049804   1.526365   5.274 2.16e-07 ***\nyear2006                7.865085   1.521659   5.169 3.69e-07 ***\nyear2007                7.320005   1.484491   4.931 1.19e-06 ***\nyear2008                7.695709   1.508191   5.103 5.13e-07 ***\nyear2009                8.741259   1.518804   5.755 1.69e-08 ***\nyear2010                8.388088   1.487661   5.638 3.19e-08 ***\nyear2011                8.787078   1.525340   5.761 1.64e-08 ***\nyear2012                9.121104   1.514125   6.024 3.78e-09 ***\nyear2013                8.949772   1.491692   6.000 4.34e-09 ***\nyear2014                8.730829   1.519104   5.747 1.77e-08 ***\nyear2015                8.248437   1.512792   5.452 8.59e-08 ***\nyear2016                7.999621   1.495341   5.350 1.47e-07 ***\nyear2017                7.881164   1.522311   5.177 3.53e-07 ***\nyear2018                7.639807   1.516810   5.037 7.10e-07 ***\nyear2019                7.610938   1.526318   4.986 9.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.972 on 411 degrees of freedom\n  (1386 observations deleted due to missingness)\nMultiple R-squared:  0.7877,    Adjusted R-squared:  0.7526 \nF-statistic: 22.43 on 68 and 411 DF,  p-value: &lt; 2.2e-16\n\n\nNow, let’s check if the heteroscedasticity problem persists.\n\nplot(model_fe, which = 1)\n\n\n\n\n\n\n\n\nDraw the same graph using ggplot(). What do you think, is there a problem?\nLet’s conduct a formal test using bptest() from lmtest library. It stands for Breusch-Pagan Test. How would you approach interpreting the results?\n\nlibrary(lmtest)\nbptest(model_fe)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_fe\nBP = 182.68, df = 68, p-value = 1.928e-12\n\n\n\n\nRobust Standard Errors\nNow, introduce robust SEs. They are also referred to as Heteroskedasticity-consistent standard errors (HC SEs).\nFirst, load the library estimatr. If you don’t have it, take a moment to install.\n\nlibrary(estimatr)\n\nNow, we can use lm_robust() function to introduce the robust standard errors to attempt to account for the heteroscedasticity.\n\nmodel_hc = lm_robust(prefisc_gini ~ openc + servadmi_pmp, cpds,\n                     fixed_effects = country + year,\n                     se_type = \"HC2\")\nsummary(model_hc)\n\n\nCall:\nlm_robust(formula = prefisc_gini ~ openc + servadmi_pmp, data = cpds, \n    fixed_effects = country + year, se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  CI Lower CI Upper  DF\nopenc         0.01157   0.004431   2.611 0.009364  0.002858  0.02028 411\nservadmi_pmp  2.44631   1.769259   1.383 0.167516 -1.031621  5.92423 411\n\nMultiple R-squared:  0.7877 ,   Adjusted R-squared:  0.7526\nMultiple R-squared (proj. model):  0.01949 ,    Adjusted R-squared (proj. model):  -0.1427 \nF-statistic (proj. model): 4.644 on 2 and 411 DF,  p-value: 0.01013\n\n\nLet’s compare two models side by side. What’s the difference? Did we account for heteroscedasticity? Remember that we have not displayed estimates for fixed effects.\n\nlibrary(modelsummary)\nmodelsummary(list(\"Fixed Effects Model\" = model_fe, \n                  \"Fixed Effects Model (HC)\" = model_hc), \n             stars = T,\n             coef_omit = \"country|year\",\n             gof_omit = \"AIC|BIC|Log.Lik.|RMSE\",\n             coef_rename = c(\"Intercept\",\n                             \"Economy Openness\",\n                             \"Public Sector\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Fixed Effects Model\n                Fixed Effects Model (HC)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  Intercept\n                  35.437***\n                  \n                \n                \n                  \n                  (1.561)\n                  \n                \n                \n                  Economy Openness\n                  0.012*\n                  0.012**\n                \n                \n                  \n                  (0.005)\n                  (0.004)\n                \n                \n                  Public Sector\n                  2.446\n                  2.446\n                \n                \n                  \n                  (1.717)\n                  (1.769)\n                \n                \n                  Num.Obs.\n                  480\n                  480\n                \n                \n                  R2\n                  0.788\n                  0.788\n                \n                \n                  R2 Adj.\n                  0.753\n                  0.753\n                \n        \n      \n    \n\n\n\n\n\nClustered Standard Errors\nDuring the lecture we have discussed clustered standard errors. Let’s try to implement it.\nAssuming there is a correlation between country X in year \\(t\\) and year \\(t+1\\), it’s valid to introduce clustered standard errors to the model. However, be careful if you don’t have enough obervations. Note a couple of things:\n\n\\(R^2\\) did not change\n\\(\\beta\\) coefficients did not change\n\n\nmodel_clust = lm_robust(prefisc_gini ~ openc + servadmi_pmp, cpds,\n                      fixed_effects = country + year,\n                      clusters = country)\n\nmodelsummary(list(\"Fixed Effects Model\" = model_fe, \n                  \"Fixed Effects Model (HC)\" = model_hc,\n                  \"Fixed Effects Model (HC + Cl)\" = model_clust), \n             stars = T,\n             coef_omit = \"country|year\",\n             gof_omit = \"AIC|BIC|Log.Lik.|RMSE\",\n             coef_rename = c(\"Intercept\",\n                             \"Economy Openness\",\n                             \"Public Sector\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Fixed Effects Model\n                Fixed Effects Model (HC)\n                Fixed Effects Model (HC + Cl)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  Intercept\n                  35.437***\n                  \n                  \n                \n                \n                  \n                  (1.561)\n                  \n                  \n                \n                \n                  Economy Openness\n                  0.012*\n                  0.012**\n                  0.012\n                \n                \n                  \n                  (0.005)\n                  (0.004)\n                  (0.021)\n                \n                \n                  Public Sector\n                  2.446\n                  2.446\n                  2.446\n                \n                \n                  \n                  (1.717)\n                  (1.769)\n                  (2.180)\n                \n                \n                  Num.Obs.\n                  480\n                  480\n                  480\n                \n                \n                  R2\n                  0.788\n                  0.788\n                  0.788\n                \n                \n                  R2 Adj.\n                  0.753\n                  0.753\n                  0.753\n                \n                \n                  Std.Errors\n                  \n                  \n                  by: country\n                \n        \n      \n    \n\n\n\nLet’s compare the models visually. First, extract the information using tidy(). The code below is a bit confusing, take a moment in your free time to get a sense what’s going on.\n\nlibrary(broom)\nmodel_comparison = bind_rows(cbind(model = \"Fixed Effects (HC)\", tidy(model_hc, conf.int = T)),\n                             cbind(model = \"Fixed Effects (HC + Cl)\", tidy(model_clust, conf.int = T)))\nmodel_comparison\n\n                    model         term   estimate   std.error statistic\n1      Fixed Effects (HC)        openc 0.01156786 0.004430807 2.6107801\n2      Fixed Effects (HC) servadmi_pmp 2.44630535 1.769259191 1.3826721\n3 Fixed Effects (HC + Cl)        openc 0.01156786 0.021259151 0.5441357\n4 Fixed Effects (HC + Cl) servadmi_pmp 2.44630535 2.179715863 1.1223047\n      p.value     conf.low  conf.high         df      outcome\n1 0.009363945  0.002857992 0.02027773 411.000000 prefisc_gini\n2 0.167516215 -1.031620650 5.92423134 411.000000 prefisc_gini\n3 0.642254837 -0.082433704 0.10556943   1.944857 prefisc_gini\n4 0.292747596 -2.537707419 7.43031811   8.410874 prefisc_gini\n\n\nThis is a variation of an effects plot, but instead of comparing different estimates, we compare the estimate across models. Clustering standard errors has widened our confidence intervals, which now cross zero.\n\nmodel_comparison %&gt;%\n  filter(term == \"openc\") %&gt;%\n  ggplot(aes(y = model, x = estimate)) + \n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, height = 0.1), position = \"dodge\") + \n  geom_vline(xintercept = 0, linetype = 2) + \n  geom_point(position = position_dodge(width = 0.1)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nHowever, a small note on robust standard errors: they are not a panacea. If faced with heteroscedasticity, consider reporting different model specifications. Similarly, as we did—try out different standard errors, clustering methods, and assess how robust your model is. If your results do not hold across different model specifications, this is a signal that there may be no effect or an indication to explore alternative solutions.\n\nUse another model (e.g., if you’re dealing with binary dependent variable, you can use logit insted of linear probability model)\nTransform DV\n\n\n\nExercises\nLet’s continue exploring CPDS dataset. The goal to identify what factors are associated with right-wing parties popularity. Say, you operatianlize it as parliamentary seat share of right-wing parties in government (gov_right3).\nIs political fragmentation associated with right-wing popularity? Quite frequently political fragmentation is measured using the effective number of parties index. Feel free to explore it in your free time here. CPDS has index for this: effpar_leg.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nSet up a simple linear regression. Set gov_right3 as dependent variable, and effpar_leg as independent. Present the summary. Take a moment to interpret the results.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nLet’s check if the homoscedasticity assumption is satisfied. First, plot the residuals vs fitted graph eather using plot() or ggplot(). Does the graph show homoscedasticity?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nLet’s formally test it. Run Breusch-Pagan test (bptest() from lmtest library). Interpret the results.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nOk, apparently there is something wrong with the data. Let’s first understand the relationship between dependent and independent variable. Using geom_point() plot effpar_leg against gov_right3.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nDoesn’t look linear, right? Let’s experiment. We see that there is a high zero inflation. We don’t know how to solve it yet, so let’s get rid of all zeroes for gov_right3. Using filter(), leave observations where gov_right3 is not equal to 0.\n\n\n\n\n\n\nSolution\n\n\n\n\ncpds_nozeroes = ...\n\n\n\nNow, create a new variable sqrt_effpar_leg that would be a square root of effpar_leg. You can use mutate() for this task.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nThen, draw a scatterplot, where sqrt_effpar_leg is on the X axis, and effpar_leg on the Y. Draw a regression line. Were we able to make the relationsip linear?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nSet up a model, where gov_right3 is dependent variable, and sqrt_effpar_leg is independent. Present the summary. Compare this adjusted model to the previous one.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nLet’s check if the homoscedasticity assumption is satisfied for the adjusted model. Plot the residuals vs fitted graph eather using plot() or ggplot(). How does it compare?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nNow, using lm_robust() cluster standard errors by country. Present summary. How did the results change? Pay special attention to the standard error.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nAdd the country and year fixed effects. Make sure that year is of class factor. How did p-value change? Why?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nPresent the results for the models using modelsummary(). Indicate confidence intervals. Are the models robust? What do we account for, and what don’t we?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\n\n\nCheck List\n I know what the standard error is\n I remember homoscedasticity assumption\n I know when I might need to use robust standard errors\n I know when I might need to use clustered standard errors\n I know how to use lm_robust() to account for fixed effects and various standard errors"
  },
  {
    "objectID": "ps405-d_6.html#footnotes",
    "href": "ps405-d_6.html#footnotes",
    "title": "Practice and Replication",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe material for this lab is based on the University of Mannheim’s Quantitative Methods class. The paper replicated is Ward, G., 2020. Happiness and voting: Evidence from four decades of elections in Europe. American Journal of Political Science, 64(3), pp.504-518.↩︎"
  },
  {
    "objectID": "ps405-d_6.html#unit-fixed-effects",
    "href": "ps405-d_6.html#unit-fixed-effects",
    "title": "Practice and Replication",
    "section": "Unit fixed effects",
    "text": "Unit fixed effects\nFirst of all, the fixed effects should be of class factor(). Let’s check it and correct if needed.\n\nclass(dta$country)\n\n[1] \"character\"\n\ndta$country = as.factor(dta$country)\n\nUnit fixed effects regression models can be very useful to solve one of the problems we already identified: omitted confounders on some higher level unit of observation. This higher level unit, in our case, are countries. Thus, let’s add country fixed effects. Notably, the variable satislfe_survey_mean.\n\nfe_model = lm(vote_share_cab ~ satislfe_survey_mean + country, data = dta)\n\nsummary(fe_model)\n\n\nCall:\nlm(formula = vote_share_cab ~ satislfe_survey_mean + country, \n    data = dta)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.5698  -3.3365   0.7488   4.0599  20.5531 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -4.1445    17.3984  -0.238 0.812112    \nsatislfe_survey_mean  18.7915     5.5556   3.382 0.000964 ***\ncountryBEL            -4.9655     3.5399  -1.403 0.163220    \ncountryDEU            -2.9294     3.6301  -0.807 0.421237    \ncountryDNK           -25.9045     4.2882  -6.041 1.68e-08 ***\ncountryESP           -12.4643     3.9014  -3.195 0.001778 ** \ncountryFIN            -0.3649     4.6387  -0.079 0.937433    \ncountryFRA           -15.7479     3.9968  -3.940 0.000136 ***\ncountryGBR           -18.0236     3.7206  -4.844 3.74e-06 ***\ncountryGRC            -4.0953     4.7534  -0.862 0.390613    \ncountryIRL           -14.2652     3.6413  -3.918 0.000147 ***\ncountryITA            -4.0091     4.2554  -0.942 0.347980    \ncountryLUX            -4.2656     3.9385  -1.083 0.280907    \ncountryNLD           -16.5458     3.9249  -4.216 4.78e-05 ***\ncountryPRT            -3.6596     4.7765  -0.766 0.445044    \ncountrySWE           -19.3069     4.6118  -4.186 5.35e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.133 on 123 degrees of freedom\nMultiple R-squared:  0.4973,    Adjusted R-squared:  0.436 \nF-statistic: 8.113 on 15 and 123 DF,  p-value: 1.679e-12\n\n\nLet’s visualize the model. Load the ggeffects library.\n\nlibrary(ggeffects)\n\nWarning: package 'ggeffects' was built under R version 4.4.2\n\n\nTo simplify the perception, let’s choose only the following countries: AUT, DNK, IRL. You can see the effect of adding the country variable quite vividly, right? Compare it to the previous graph.\n\nggpredict(..., terms = c(\"...\", \"... [AUT, DNK, IRL]\")) %&gt;%\n  plot() \n\nIncluding country fixed effects has the following pros and cons:\n\nAdvantage: Confounders that only vary between countries are of no more concern.\nDisadvantage: We lose all the higher level information. So if we were interested in why governing parties in country A seem to systematically receive more votes than governing parties in B, then we cannot answer them anymore as soon as we include country fixed effects. Why? Perfect colinearity between country fixed effects and time-invariant country characteristics (e.g, political system)"
  },
  {
    "objectID": "ps405-d_6.html#two-way-fixed-effects",
    "href": "ps405-d_6.html#two-way-fixed-effects",
    "title": "Practice and Replication",
    "section": "Two way fixed effects",
    "text": "Two way fixed effects\nNow, let’s add year fixed effects. Usually, when we have two fixed effects, we refer to it as two way fixed effects.\nAgain, the fixed effects should be of class factor(). Let’s check it and correct if needed.\n\nclass(dta$year)\n\n[1] \"numeric\"\n\ndta$year = as.factor(dta$year)\n\nJust as country fixed effects help us to account for all unobserved confounders that only vary across countries (but not within countries), time fixed effects help us to account for all unobserved confounders that only vary across time (but not within time, i.e. between countries at one point in time).\n\ntwfe_model = lm(vote_share_cab ~ satislfe_survey_mean + year + country, data = dta)\n\nsummary(twfe_model)\n\n\nCall:\nlm(formula = vote_share_cab ~ satislfe_survey_mean + year + country, \n    data = dta)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.706  -2.384   0.000   2.998  16.267 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -28.422     21.550  -1.319 0.190868    \nsatislfe_survey_mean   23.277      6.744   3.452 0.000883 ***\nyear1974               17.376      8.051   2.158 0.033832 *  \nyear1975               -2.300      9.568  -0.240 0.810632    \nyear1976               13.916      8.764   1.588 0.116159    \nyear1977               10.132      7.836   1.293 0.199644    \nyear1978                7.845      8.841   0.887 0.377457    \nyear1979               15.994      7.814   2.047 0.043886 *  \nyear1980               16.637     10.084   1.650 0.102804    \nyear1981               11.353      7.621   1.490 0.140136    \nyear1982                9.686      8.324   1.164 0.247988    \nyear1983               19.800      8.243   2.402 0.018560 *  \nyear1984               15.034      8.459   1.777 0.079235 .  \nyear1985               12.587      8.853   1.422 0.158894    \nyear1986               14.395      8.283   1.738 0.085977 .  \nyear1987               12.513      7.516   1.665 0.099740 .  \nyear1988                9.391      8.511   1.103 0.273085    \nyear1989               12.756      7.890   1.617 0.109763    \nyear1990               10.808      8.507   1.270 0.207502    \nyear1991               16.207      8.921   1.817 0.072918 .  \nyear1992               14.106      8.310   1.697 0.093404 .  \nyear1993                6.628      8.332   0.796 0.428601    \nyear1994               13.128      7.845   1.673 0.098067 .  \nyear1995               11.993      8.353   1.436 0.154896    \nyear1996               13.466      8.889   1.515 0.133614    \nyear1997                6.247      8.266   0.756 0.451941    \nyear1998               13.457      7.840   1.716 0.089872 .  \nyear1999               14.511      7.855   1.847 0.068297 .  \nyear2000               11.505      9.075   1.268 0.208460    \nyear2001               10.043      8.114   1.238 0.219377    \nyear2002                8.987      7.665   1.172 0.244415    \nyear2003               11.397      8.285   1.376 0.172684    \nyear2004                9.556      8.362   1.143 0.256456    \nyear2005                5.740      7.931   0.724 0.471288    \nyear2006                5.110      8.076   0.633 0.528670    \nyear2007                7.857      7.727   1.017 0.312219    \nyear2008               12.285      8.343   1.472 0.144733    \nyear2009               14.633      7.962   1.838 0.069694 .  \nyear2010                2.023      8.086   0.250 0.803072    \nyear2011               -2.143      7.727  -0.277 0.782207    \nyear2012                7.787      8.270   0.942 0.349149    \nyear2013                9.693      8.295   1.169 0.245988    \nyear2014               11.236      8.940   1.257 0.212381    \ncountryBEL             -6.146      3.853  -1.595 0.114542    \ncountryDEU             -4.945      4.000  -1.236 0.219819    \ncountryDNK            -26.270      5.079  -5.172 1.61e-06 ***\ncountryESP            -11.341      4.437  -2.556 0.012428 *  \ncountryFIN              1.702      5.034   0.338 0.736101    \ncountryFRA            -13.166      4.581  -2.874 0.005159 ** \ncountryGBR            -20.140      4.225  -4.767 8.00e-06 ***\ncountryGRC             -2.053      5.628  -0.365 0.716261    \ncountryIRL            -13.351      4.074  -3.278 0.001537 ** \ncountryITA             -5.090      4.602  -1.106 0.271927    \ncountryLUX             -8.416      4.344  -1.937 0.056159 .  \ncountryNLD            -17.457      4.400  -3.967 0.000155 ***\ncountryPRT             -1.255      5.315  -0.236 0.813944    \ncountrySWE            -18.345      4.951  -3.705 0.000382 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.766 on 82 degrees of freedom\nMultiple R-squared:  0.6985,    Adjusted R-squared:  0.4926 \nF-statistic: 3.392 on 56 and 82 DF,  p-value: 2.595e-07\n\n\nLet’s visualize to get the sense of what’s going on. Check the following countries AUT, DNK, IRL in the following years 1987, 1992, 2011. As it is a fixed effects model, we can interpret the same style as “holding everything else constant, on average one unit increase …”.\n\nggpredict(twfe_model, terms = c(\"satislfe_survey_mean\", \"country [AUT, DNK, IRL]\", \"year [1987, 1992, 2011]\")) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nUsually, we do not report estimates for the fixed effects. You would expect to see something like this:\n\nlibrary(modelsummary)\n\nWarning: package 'modelsummary' was built under R version 4.4.2\n\nmodelsummary(list(\"Pooled Model\" = lm_pooling,\n                  \"Fixed Effects Model\" = fe_model,\n                  \"Two-Way Fixed Effects Model\" = twfe_model),\n             coef_omit = \"country|year\",\n             stars = TRUE,\n             gof_omit = \"AIC|BIC|Log.Lik.|F|RMSE\",\n             add_rows = data.frame(term = \"Fixed effects\",\n                                   pooled = \"N\",\n                                   fixed = \"Y\",\n                                   twfe = \"Y\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Pooled Model\n                Fixed Effects Model\n                Two-Way Fixed Effects Model\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)         \n                  31.040***\n                  -4.145   \n                  -28.422  \n                \n                \n                                      \n                  (8.025)  \n                  (17.398) \n                  (21.550) \n                \n                \n                  satislfe_survey_mean\n                  3.936    \n                  18.792***\n                  23.277***\n                \n                \n                                      \n                  (2.580)  \n                  (5.556)  \n                  (6.744)  \n                \n                \n                  Num.Obs.            \n                  139      \n                  139      \n                  139      \n                \n                \n                  R2                  \n                  0.017    \n                  0.497    \n                  0.698    \n                \n                \n                  R2 Adj.             \n                  0.010    \n                  0.436    \n                  0.493    \n                \n                \n                  Fixed effects       \n                  N        \n                  Y        \n                  Y"
  },
  {
    "objectID": "ps405-d_6.html#including-control-variables",
    "href": "ps405-d_6.html#including-control-variables",
    "title": "Practice and Replication",
    "section": "Including control variables",
    "text": "Including control variables\nBy including country and year fixed effects, we made sure that our effect estimate is not biased due to unobserved confounders that are time-invariant on the country level in a given year. However, we still need to think about potential time-variant confounders we need to control for. Take a moment to think this through.\nHere, we are back in the familiar game: we must think about potential covariates, try to get the data and include them in our model. Ward includes a series of control variables:\n\nparties_ingov = Number of parties in government\nseatshare_cabinet = % seats held by governing coalition\ncab_ideol_sd = Gov Ideologial Disparity (Standard deviation of government party positions on left-right scale)\nENEP_tmin1 = Party Fractionalisation - Last Election (Gallagher Fractionalization Index)\n\nLet us set up the model.\n\ntwfe_controls_model = lm(vote_share_cab ~ satislfe_survey_mean + year + country + parties_ingov + seatshare_cabinet + cab_ideol_sd + ENEP_tmin1, data = dta)\n\nBy now, the summary below is unreadable.\n\nsummary(twfe_controls_model)\n\nLet’s play around the modelsummary() to include the most important information.\n\nmodelsummary(twfe_controls_model,\n             coef_omit = \"country|year\",\n             stars = TRUE,\n             output = \"markdown\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)         \n                  -46.353* \n                \n                \n                                      \n                  (22.003) \n                \n                \n                  satislfe_survey_mean\n                  24.545***\n                \n                \n                                      \n                  (6.325)  \n                \n                \n                  parties_ingov       \n                  3.222*   \n                \n                \n                                      \n                  (1.231)  \n                \n                \n                  seatshare_cabinet   \n                  22.265** \n                \n                \n                                      \n                  (8.411)  \n                \n                \n                  cab_ideol_sd        \n                  -0.825   \n                \n                \n                                      \n                  (1.359)  \n                \n                \n                  ENEP_tmin1          \n                  -0.069   \n                \n                \n                                      \n                  (0.897)  \n                \n                \n                  Num.Obs.            \n                  139      \n                \n                \n                  R2                  \n                  0.766    \n                \n                \n                  R2 Adj.             \n                  0.587    \n                \n                \n                  AIC                 \n                  941.1    \n                \n                \n                  BIC                 \n                  1123.0   \n                \n                \n                  Log.Lik.            \n                  -408.542 \n                \n                \n                  F                   \n                  4.267    \n                \n                \n                  RMSE                \n                  4.57     \n                \n        \n      \n    \n\n\n\nHowever, often authors present only the main explanatory variable. You should mention your controls, but you don’t have to report it directly (usually, in this case the extended table goes to appendix). Let’s present all of the models. And customize the display. This is what you can see in Table 1(column 1) of the article with one difference. The author standardized numeric variables. Why would you standardize the variable in this case?\n\nmodelsummary(list(\"Pooled Model\" = lm_pooling,\n                  \"Fixed Effects Model\" = fe_model,\n                  \"Two-Way Fixed Effects Model\" = twfe_model,\n                  \"Two-Way Fixed Effects Model w Controls\" = twfe_controls_model),\n             coef_rename = c(\"(Intercept)\", \"Well-Being\"),\n             coef_omit = \"country|year|parties_ingov|seatshare_cabinet|cab_ideol_sd|ENEP_tmin1\",\n             stars = TRUE,\n             gof_omit = \"AIC|BIC|Log.Lik.|F|RMSE\",\n             add_rows = data.frame(term = c(\"Fixed effects\", \"Conrols\"),\n                                   pooled = c(\"N\", \"N\"),\n                                   fixed = c(\"Y\", \"N\"),\n                                   twfe = c(\"Y\", \"N\"),\n                                   twfe_c = c(\"Y\", \"Y\")))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Pooled Model\n                Fixed Effects Model\n                Two-Way Fixed Effects Model\n                Two-Way Fixed Effects Model w Controls\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)  \n                  31.040***\n                  -4.145   \n                  -28.422  \n                  -46.353* \n                \n                \n                               \n                  (8.025)  \n                  (17.398) \n                  (21.550) \n                  (22.003) \n                \n                \n                  Well-Being   \n                  3.936    \n                  18.792***\n                  23.277***\n                  24.545***\n                \n                \n                               \n                  (2.580)  \n                  (5.556)  \n                  (6.744)  \n                  (6.325)  \n                \n                \n                  Num.Obs.     \n                  139      \n                  139      \n                  139      \n                  139      \n                \n                \n                  R2           \n                  0.017    \n                  0.497    \n                  0.698    \n                  0.766    \n                \n                \n                  R2 Adj.      \n                  0.010    \n                  0.436    \n                  0.493    \n                  0.587    \n                \n                \n                  Fixed effects\n                  N        \n                  Y        \n                  Y        \n                  Y        \n                \n                \n                  Conrols      \n                  N        \n                  N        \n                  N        \n                  Y"
  },
  {
    "objectID": "ds7_drafts.html",
    "href": "ds7_drafts.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(estimatr)\nlibrary(modelsummary)\n\ndf = read_excel(\"data/cpds.xlsx\")\n\noutlays Total outlays (disbursements) of general government as a percentage of GDP\ngov_right1 Government composition: cabinet posts of right-wing parties in percentage of total cabinet posts.\nprefisc_gini ~ openc + servadmi_pmp + year + country\n\nprefisc_gini - Gini index\nopenc - Openness of the economy\nservadmi_pmp - Public and mandatory private employment services and administration as a percentage of GDP.\n\n\n# df$outlays = as.numeric(df$outlays)\n# df$emprot_reg = as.numeric(df$emprot_reg)\n# df$openc = as.numeric(df$openc)\n\nmodel = lm(prefisc_gini ~ openc + servadmi_pmp + country + year, df)\nmodel_rob = lm_robust(prefisc_gini ~ openc + servadmi_pmp + country + year, df, se_type = \"HC2\")\n\nsummary(model)\n\n\nCall:\nlm(formula = prefisc_gini ~ openc + servadmi_pmp + country + \n    year, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6167 -1.1213 -0.0129  1.0697  8.3904 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -2.392e+02  2.832e+01  -8.446 4.23e-16 ***\nopenc                  5.849e-03  4.878e-03   1.199 0.231173    \nservadmi_pmp           3.886e+00  1.771e+00   2.194 0.028766 *  \ncountryAustria        -2.962e+00  8.179e-01  -3.621 0.000326 ***\ncountryBelgium        -3.000e+00  9.366e-01  -3.203 0.001455 ** \ncountryCanada         -1.432e+00  7.575e-01  -1.891 0.059262 .  \ncountryCzech Republic -5.101e+00  1.067e+00  -4.780 2.38e-06 ***\ncountryDenmark        -4.587e+00  9.417e-01  -4.871 1.54e-06 ***\ncountryEstonia        -3.379e+00  1.258e+00  -2.687 0.007488 ** \ncountryFinland        -1.921e+00  9.794e-01  -1.961 0.050483 .  \ncountryFrance          3.089e-01  7.939e-01   0.389 0.697404    \ncountryGermany        -2.872e+00  7.833e-01  -3.666 0.000275 ***\ncountryGreece          3.489e-01  1.072e+00   0.326 0.744893    \ncountryHungary         2.485e+00  1.134e+00   2.192 0.028910 *  \ncountryIceland        -1.023e+01  1.426e+00  -7.175 3.00e-12 ***\ncountryIreland         3.299e+00  1.007e+00   3.276 0.001136 ** \ncountryItaly          -6.247e-01  1.060e+00  -0.589 0.556052    \ncountryJapan          -6.761e+00  1.423e+00  -4.750 2.74e-06 ***\ncountryLuxembourg     -5.159e+00  1.374e+00  -3.756 0.000195 ***\ncountryNetherlands    -3.630e+00  9.987e-01  -3.635 0.000310 ***\ncountryNorway         -5.637e+00  9.541e-01  -5.908 6.85e-09 ***\ncountryPoland          5.099e-01  8.498e-01   0.600 0.548844    \ncountryRomania        -2.886e+00  8.937e-01  -3.230 0.001331 ** \ncountrySlovakia       -8.838e+00  1.085e+00  -8.143 3.84e-15 ***\ncountrySlovenia       -7.001e+00  1.240e+00  -5.646 2.92e-08 ***\ncountrySpain          -1.942e-02  7.787e-01  -0.025 0.980114    \ncountrySweden         -4.883e+00  8.263e-01  -5.910 6.78e-09 ***\ncountrySwitzerland    -9.936e+00  8.845e-01 -11.234  &lt; 2e-16 ***\ncountryUnited Kingdom  1.566e+00  7.638e-01   2.051 0.040883 *  \ncountryUSA             2.588e+00  7.826e-01   3.307 0.001019 ** \nyear                   1.406e-01  1.421e-02   9.891  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.154 on 449 degrees of freedom\n  (1386 observations deleted due to missingness)\nMultiple R-squared:  0.7234,    Adjusted R-squared:  0.7049 \nF-statistic: 39.14 on 30 and 449 DF,  p-value: &lt; 2.2e-16\n\nsummary(model_rob) \n\n\nCall:\nlm_robust(formula = prefisc_gini ~ openc + servadmi_pmp + country + \n    year, data = df, se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n                        Estimate Std. Error   t value  Pr(&gt;|t|)   CI Lower\n(Intercept)           -2.392e+02  29.331889  -8.15376 3.557e-15 -2.968e+02\nopenc                  5.849e-03   0.004936   1.18501 2.366e-01 -3.851e-03\nservadmi_pmp           3.886e+00   2.164633   1.79513 7.331e-02 -3.683e-01\ncountryAustria        -2.962e+00   0.618025  -4.79280 2.239e-06 -4.177e+00\ncountryBelgium        -3.000e+00   0.805828  -3.72316 2.218e-04 -4.584e+00\ncountryCanada         -1.432e+00   0.598748  -2.39230 1.715e-02 -2.609e+00\ncountryCzech Republic -5.101e+00   1.085552  -4.69876 3.486e-06 -7.234e+00\ncountryDenmark        -4.587e+00   0.691593  -6.63265 9.500e-11 -5.946e+00\ncountryEstonia        -3.379e+00   1.714363  -1.97075 4.937e-02 -6.748e+00\ncountryFinland        -1.921e+00   0.751229  -2.55681 1.089e-02 -3.397e+00\ncountryFrance          3.089e-01   0.563052   0.54861 5.835e-01 -7.976e-01\ncountryGermany        -2.872e+00   0.651152  -4.41071 1.291e-05 -4.152e+00\ncountryGreece          3.489e-01   1.153005   0.30257 7.624e-01 -1.917e+00\ncountryHungary         2.485e+00   1.673774   1.48493 1.383e-01 -8.040e-01\ncountryIceland        -1.023e+01   0.787229 -12.99662 5.260e-33 -1.178e+01\ncountryIreland         3.299e+00   1.071078   3.08010 2.196e-03  1.194e+00\ncountryItaly          -6.247e-01   1.042148  -0.59942 5.492e-01 -2.673e+00\ncountryJapan          -6.761e+00   1.084724  -6.23316 1.055e-09 -8.893e+00\ncountryLuxembourg     -5.159e+00   1.372398  -3.75941 1.928e-04 -7.857e+00\ncountryNetherlands    -3.630e+00   1.191594  -3.04647 2.452e-03 -5.972e+00\ncountryNorway         -5.637e+00   1.023302  -5.50831 6.111e-08 -7.648e+00\ncountryPoland          5.099e-01   0.906822   0.56225 5.742e-01 -1.272e+00\ncountryRomania        -2.886e+00   0.643268  -4.48709 9.185e-06 -4.151e+00\ncountrySlovakia       -8.838e+00   1.508878  -5.85726 9.106e-09 -1.180e+01\ncountrySlovenia       -7.001e+00   1.179537  -5.93496 5.883e-09 -9.319e+00\ncountrySpain          -1.942e-02   0.711831  -0.02728 9.782e-01 -1.418e+00\ncountrySweden         -4.883e+00   0.653125  -7.47685 4.028e-13 -6.167e+00\ncountrySwitzerland    -9.936e+00   0.656432 -15.13652 4.084e-42 -1.123e+01\ncountryUnited Kingdom  1.566e+00   0.651164   2.40549 1.655e-02  2.867e-01\ncountryUSA             2.588e+00   0.617289   4.19257 3.324e-05  1.375e+00\nyear                   1.406e-01   0.014697   9.56416 7.555e-20  1.117e-01\n                        CI Upper  DF\n(Intercept)           -1.815e+02 449\nopenc                  1.555e-02 449\nservadmi_pmp           8.140e+00 449\ncountryAustria        -1.747e+00 449\ncountryBelgium        -1.417e+00 449\ncountryCanada         -2.557e-01 449\ncountryCzech Republic -2.967e+00 449\ncountryDenmark        -3.228e+00 449\ncountryEstonia        -9.418e-03 449\ncountryFinland        -4.444e-01 449\ncountryFrance          1.415e+00 449\ncountryGermany        -1.592e+00 449\ncountryGreece          2.615e+00 449\ncountryHungary         5.775e+00 449\ncountryIceland        -8.684e+00 449\ncountryIreland         5.404e+00 449\ncountryItaly           1.423e+00 449\ncountryJapan          -4.629e+00 449\ncountryLuxembourg     -2.462e+00 449\ncountryNetherlands    -1.288e+00 449\ncountryNorway         -3.626e+00 449\ncountryPoland          2.292e+00 449\ncountryRomania        -1.622e+00 449\ncountrySlovakia       -5.873e+00 449\ncountrySlovenia       -4.682e+00 449\ncountrySpain           1.380e+00 449\ncountrySweden         -3.600e+00 449\ncountrySwitzerland    -8.646e+00 449\ncountryUnited Kingdom  2.846e+00 449\ncountryUSA             3.801e+00 449\nyear                   1.694e-01 449\n\nMultiple R-squared:  0.7234 ,   Adjusted R-squared:  0.7049 \nF-statistic: 104.2 on 30 and 449 DF,  p-value: &lt; 2.2e-16\n\nmodelsummary(list(model, model_rob), \n             stars = T,\n             coef_omit = c(\"country|year\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  -239.165***\n                  -239.165***\n                \n                \n                  \n                  (28.318)\n                  (29.332)\n                \n                \n                  openc\n                  0.006\n                  0.006\n                \n                \n                  \n                  (0.005)\n                  (0.005)\n                \n                \n                  servadmi_pmp\n                  3.886*\n                  3.886+\n                \n                \n                  \n                  (1.771)\n                  (2.165)\n                \n                \n                  Num.Obs.\n                  480\n                  480\n                \n                \n                  R2\n                  0.723\n                  0.723\n                \n                \n                  R2 Adj.\n                  0.705\n                  0.705\n                \n                \n                  AIC\n                  2130.8\n                  2130.8\n                \n                \n                  BIC\n                  2264.3\n                  2264.3\n                \n                \n                  Log.Lik.\n                  -1033.379\n                  \n                \n                \n                  RMSE\n                  2.08\n                  2.08\n                \n        \n      \n    \n\n\n\n\nplot(model, 1)\n\n\n\n\n\n\n\nplot(model, 2)\n\n\n\n\n\n\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 113.83, df = 30, p-value = 1.082e-11\n\n\n\nggplot() +\n  geom_point(aes(y = model$residuals,\n                 x = predict(model))) +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\ncluster: thus, we can control for the confounders on the country level"
  },
  {
    "objectID": "ps405-d_8.html",
    "href": "ps405-d_8.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Before we start\n\nAny questions?\nWe’re almost done with the class! What you think we should recap? Feel free to leave any thoughts in this form after the class. When thinking about what we have covered, don’t forget that each lab has “Check list” in the end.\nTo get the extra points for the exercises you will need to complete only 5 exercises.\n\n\nDownload script\n\n\n\nDownload data\n\n\n\nReview of the previous week\n\n\n\n\n\n\nReview\n\n\n\nLoad tidyverse library\n\nlibrary(tidyverse)\n\nLoad the Comparative Political Dataset data (cpds.xlsx).\nLet’s attempt to explain Total expenditure on health as a percentage of GDP (health_pmp). Regress it against Total labour force as a percentage of population (labfopar) and Total reported union members (grossu). Set up a model using lm().\nUsing plot() present residuals vs fitted graph. Is it homoscedastic?\nAdd country and year fixed effects. Don’t forget about the variables’ class!\nNow, plot the residuals vs fitted. How did it change? Is it homoscedastic?\nUsing bptest() from lmtest library test your assumption. Interpret the result.\nUsing lm_robust() from estimatr library run the same model with HC2 standard errors. Did the result change?\n\nmodel_se = ...\n\nRun the same model, but cluster the standard errors by country.\n\nmodel_cl = ...\n\nUsing modelsummary() present three models: with fixed effects, with fixed effects and robust standard errors, and with fixed effects and robsut standard errors clustered by country. Briefly compare them.\n\n\n\n\nAgenda\n\nRefreshing homoscedasticity assumption\nDiscuss Influential Points and outliers\nCheck for Normality of Errors\nTest models for Multicollinearity\n\n\n\nHomoscedasticity\nToday we are working with European Social Survey. You can access their documentation here. I have pre-processed the data for you. You can find how I did it here. We are interested in explaining why people trust or distrust each other.\nWe are going to use the following variables:\n\npplfair: Most people try to take advantage of you, or try to be fair (0: Most people try to take advantage of me; 10: Most people try to be fair)\ntrstplt: Trust in politicians (0: no trust; 10: complete trust)\ntrstprt: Trust in political parties\nedulvlb: Education level\n\nLoad the dataset.\n\ness = read.csv(\"data/ess.csv\")\n\nLet’s wrangle the data first. To ease the comprehension, rename the variables.\n\ness = ess %&gt;%\n  rename(trust = pplfair,\n         trust_politicians = trstplt,\n         trust_parties = trstprt,\n         education = edulvlb)\n\nhead(ess)\n\n  cntry    trust trust_politicians trust_parties   rlgdgr    happy education\n1    AT 6.385696          3.773545      3.723961 4.612917 7.781570  1.243568\n2    BE 6.076585          4.172808      3.942565 4.145283 7.778545  1.642675\n3    CH 6.431571          5.540824      5.292646 4.482909 8.154348  1.498912\n4    CY 4.428363          2.681481      2.497041 6.756598 6.969118  1.469173\n5    DE 6.177006          4.004165      3.974069 3.922599 7.762929  1.435993\n6    ES 5.578890          2.784035      2.776864 4.104518 7.847991  1.438520\n\n\nSet up the model. Analyze the regression output.\n\nmodel_trust = lm(trust ~ trust_politicians + education + trust_parties, ess)\nsummary(model_trust)\n\n\nCall:\nlm(formula = trust ~ trust_politicians + education + trust_parties, \n    data = ess)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75756 -0.26177 -0.07697  0.29635  1.03112 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)        1.01778    0.88503   1.150    0.264  \ntrust_politicians  0.04732    0.66492   0.071    0.944  \neducation          1.97227    0.73235   2.693    0.014 *\ntrust_parties      0.45800    0.66839   0.685    0.501  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4472 on 20 degrees of freedom\nMultiple R-squared:  0.749, Adjusted R-squared:  0.7114 \nF-statistic:  19.9 on 3 and 20 DF,  p-value: 3.22e-06\n\n\nLet’s plot the residuals vs fitted graph. Is it homoscedastic?\n\nplot(model_trust, 1)\n\n\n\n\n\n\n\n\nAlternatively, feel free to use ggplot() for this purpose.\n\nggplot(model_trust, aes(.fitted, .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nLet’s test our assumption. All good! Refer to the Lab 7 if you are interested in how to approach fixing the heteroscedasticity problem.\n\nlibrary(lmtest)\nbptest(model_trust)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_trust\nBP = 3.8039, df = 3, p-value = 0.2834\n\n\n\n\nInfluential Points\nInfluential points can drive our results significantly! Let’s try to find if we have any in our small dataset. Here we are going to use Cook’s Distance. Generally, there are various thresholds suggetions, but in this class we stick to the following formula:\n\\[\nD = \\frac{4}{N-k-1}\n\\] Where \\(N\\) is the number of observations and \\(k\\) is the number of covariates. You don’t have to know it, but the denominator is basically Degrees of freedom for residuals.\nCalculate the cutoff\n\ncutoff = 4 / (model_trust$df.residual)\ncutoff\n\n[1] 0.2\n\n\nCalculate the Cook’s distance for observations\n\ncooks.distance(model_trust)\n\n           1            2            3            4            5            6 \n0.2719435794 0.0145865721 0.0367187308 0.2316452893 0.0064423418 0.0166924949 \n           7            8            9           10           11           12 \n0.0148949821 0.2684953279 0.0513101291 0.0090977971 0.1002006909 0.0527893685 \n          13           14           15           16           17           18 \n0.0053112368 0.0341848255 0.0593545749 0.0015258481 0.0096398014 0.0393548089 \n          19           20           21           22           23           24 \n0.0397060110 0.0001259869 0.0126482655 0.0012011773 0.0041391820 0.0795677570 \n\n\nVisualize what we’ve got. With the given cutoff, there are three influential observations\n\nplot(model_trust, 4)\nabline(h = cutoff, lty = 2)\n\n\n\n\n\n\n\n\nAlternatively, we can use ggplot().\n\nggplot(model_trust, aes(seq_along(.cooksd), .cooksd)) +\n  geom_col() +\n  geom_hline(yintercept = cutoff)\n\n\n\n\n\n\n\n\nSet up models with and without influential points. Our goal is to understand their impact: do they drive the results? What have changed? General advice: stick to the more conservative model.\n\nlibrary(modelsummary)\n\nmodel_trust_wo = lm(trust ~ trust_politicians + education + trust_parties, ess[-c(1, 4, 8),])\n\nmodels = list(\n  \"Full Model\" = model_trust,\n  \"Without Outliers\" = model_trust_wo)\n\nmodelsummary(models, \n             gof_omit = \"AIC|BIC|Log.Lik.|F|RMSE\", \n             stars = T)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Full Model\n                Without Outliers\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.018\n                  0.125\n                \n                \n                  \n                  (0.885)\n                  (0.677)\n                \n                \n                  trust_politicians\n                  0.047\n                  -0.234\n                \n                \n                  \n                  (0.665)\n                  (0.610)\n                \n                \n                  education\n                  1.972*\n                  2.885***\n                \n                \n                  \n                  (0.732)\n                  (0.580)\n                \n                \n                  trust_parties\n                  0.458\n                  0.617\n                \n                \n                  \n                  (0.668)\n                  (0.628)\n                \n                \n                  Num.Obs.\n                  24\n                  21\n                \n                \n                  R2\n                  0.749\n                  0.871\n                \n                \n                  R2 Adj.\n                  0.711\n                  0.849\n                \n        \n      \n    \n\n\n\n\n\nNormality of Errors\nAnother important prerequisite is the normal distribution of errors (i.e., normality). Once again, we can use plot(). First, visualize plot without outliers.\n\nplot(model_trust_wo, 2)\n\n\n\n\n\n\n\n\nNow, with outliers. Compare two graphs.\n\nplot(model_trust, 2)\n\n\n\n\n\n\n\n\nHow different are they? We need to use formal tests. How would you interpret Shapiro-Wilk test below?\n\nshapiro.test(model_trust$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model_trust$residuals\nW = 0.96829, p-value = 0.6249\n\nshapiro.test(model_trust_wo$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model_trust_wo$residuals\nW = 0.94018, p-value = 0.2197\n\n\nOf course, you can visualize this with ggplot(), too. More details on visualizing base R diagnostics plots in ggplot is available here.\n\nggplot(model_trust) +\n  stat_qq(aes(sample = .stdresid)) +\n  geom_abline()\n\n\n\n\n\n\n\n\nOk, what to do if residuals are not normally distributed? Transform DV and/or IV. Quite in a similar way we did it in Lab 5. A slightly more formal way of doing this is available here.\nLet’s present the summary diagnostics.\n\npar(mfrow = c(2, 2))\nplot(model_trust)\n\n\n\n\n\n\n\n\nCheck other possible diagnostics plots below.\n\n?plot.lm()\n\n\n\nMulticollinearity\nFinally, multicollinearity. We do not want to have perfectly collinear variables. Let’s check is using ggpairs() from GGally library.\n\nlibrary(GGally)\nggpairs(ess, columns = c(\"trust\", \"trust_politicians\", \"trust_parties\", \"education\"))\n\n\n\n\n\n\n\n\nApparently, trust in politicians and trust in political parties capture overlapping concepts. However, their \\(\\rho = 0.99\\). More formally, we can calculate variance inflation factor. Use vif() function from car library. Rule of thumb is: if VIF is greater than 10, we have multicollinearity.\n\nlibrary(car)\nvif(model_trust)\n\ntrust_politicians         education     trust_parties \n        49.546656          1.503323         49.792787 \n\n\nLet’s exclude trust in political parties (trust_parties). Model specification is crucial!\n\nmodel_trust_excl = lm(trust ~ trust_politicians + education, ess)\n\n\nmodelsummary(list(\"Model Base\" = model_trust,\n                  \"Model Without Multicollinearity\" = model_trust_excl),\n             stars = T,\n             gof_omit = \"AIC|BIC|Log.Lik.|F|RMSE\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Model Base\n                Model Without Multicollinearity\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  1.018\n                  0.983\n                \n                \n                  \n                  (0.885)\n                  (0.872)\n                \n                \n                  trust_politicians\n                  0.047\n                  0.496***\n                \n                \n                  \n                  (0.665)\n                  (0.114)\n                \n                \n                  education\n                  1.972*\n                  2.010*\n                \n                \n                  \n                  (0.732)\n                  (0.721)\n                \n                \n                  trust_parties\n                  0.458\n                  \n                \n                \n                  \n                  (0.668)\n                  \n                \n                \n                  Num.Obs.\n                  24\n                  24\n                \n                \n                  R2\n                  0.749\n                  0.743\n                \n                \n                  R2 Adj.\n                  0.711\n                  0.719\n                \n        \n      \n    \n\n\n\nOk, excluded one variable. What’s next? Go over the diagnostics again with the corrected model!\n\n\nExercises\nLet’s explore other variables’ effects. First, rename the variable rlgdgr (How religious are you) to religion\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nUsing ggplot() draw distribution of religion variable\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nSet up a linear model: predict trust by trust_politicians, education and religion. Briefly describe the results: which variables are statistically significant and what is the direction of association (negative/positive).\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nDraw Cook’s distance. Use the same threshold formula as we used in the lab. Are there any influential points?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nSet up a model without outliers. How different are the results?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\n\n\nAdditional Exercises\nFor the extra credit you don’t have to do the exercises below. These are optional!\nDraw a redisuals vs fitted plot. Use either plot() or ggplot(). Is the homoscedasticity assumption satisfied?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nUsing bptest() from lmtest library, test it formally. Interpret the results\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nDraw a qqplot using plot(). Is the normality assumption satisfied?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nTest it formally using Shapiro-Wilk test (shapiro.test()). Interpret the results.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nUsing vif() from car library check if there is multicollinearity.\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\nBriefly summarize the diagnostics. What are the problems with the model? How substantive are they?\n\n\n\n\n\n\nSolution\n\n\n\nYOUR SOLUTION HERE\n\n\n\n\nCheck List\n I know how to interpret the residuals vs fitted plot, and I know how to test for homoscedasticity formally using bptest()!\n I know what Cook’s distance is, and I know that there are various formulas to calculate the threshold\n I am not afraid of using qqPlot to get the sense if residuals are distributed normally\n I know what Shapiro Wilk test, and I will use it to test if my data is distributed normally\n I know what variance inflation factor is, and I will use vif() to search for variables with value greater than 10"
  },
  {
    "objectID": "ps405-d_9.html",
    "href": "ps405-d_9.html",
    "title": "GLMs and Quarter Review",
    "section": "",
    "text": "Before we start\n\nDownload script\n\n\n\nDownload data\n\n\n\nOptional Review of the previous week\n\n\n\n\n\n\nReview\n\n\n\nLoad tidyverse library\n\nlibrary(tidyverse)\n\nLoad the European Social Value data (ess.csv).\nRename the variables:\n\npplfair to trust\ntrstplt to trust_politicians\nrlgdgr to religion\n\nRun the following model. DV: happy, IVs: trust, trust_politicians and religion. Present the summary().\nUsing bptest() from the lmtest library, test if the model is homoscedastic. Interpret the results of Breusch-Pagan test.\nDraw residuals vs fitted plot. Does it match the BP test results? You can use plot(..., which = 1).\nCalculate the cutoff for the Cook’s distance using the formula: \\(\\frac{4}{N-k-1}\\). Plot the Cook’s distance using plot() and add a cutoff line using abline(). Are there any influential points?\nSet up a model without influential points. How different the estimates are? Present a simple modelsummary() comparison.\n\nmodel_no = lm(happy ~ trust + trust_politicians + religion, ess[-c(...),])\n\nlibrary(...)\nmodelsummary(list(...),\n             stars = T)\n\nDraw a Q-Q plot using plot(..., which = 2) for the full model. Are the residuals distributed normally?\nUsing shapiro.test() test the normality of the residuals in the model. Interpret the results.\nWhat else we have to check? How would you do that? No code needed but feel free to rely on the previous lab for this question.\n\n\n\n\nAgenda\n\nRecap of what we did\nAnd a short introduction to GLM alongside\n\n\n\nRecap\nToday we are working with Transitional Justice Evaluation Tools dataset. You can explore their codebook here. Let’s load the data.\n\ntjet = read.csv(\"data/tjet.csv\")\n\nPrint out the first observations\n\n...(tjet)\n\nWe are going to use the following variables:\n\ncountry and year\ndtr democratic transition\ntcs_operated truth commission operated\ntrials_domestic count of domestic trials per country and start year\nregion United Nations region\n\nSubset the above variables. You can do this with select()\n\ntjet_sub = ... %&gt;%\n  ...(country, year, dtr, tcs_operated, trials_domestic, region) \n\nLeave more recent observations. Say, starting year 1975.\n\ntjet_sub = tjet_sub %&gt;%\n  ...(year &gt;= 1975)\n\nFirst, let’s explore the distribution of trials_domestic variable. Draw a histogram.\n\n...\n\nNow, draw a boxplot of trials_domestic by region.\n\nggplot(tjet_sub) +\n  ...(aes(x = region, y = ...))\n\nNow, present the descriptive statistics for the same variable (trials_domestic)\n\n...\n\nNow, let’s do a slightly more detailed subgroup analysis. Calculate the average and median trials_domestic for all the regions. Save that to a new object. Why is the average so different from the median?\n\ntrials_region = tjet_sub %&gt;%\n  ...(region) %&gt;%\n  ...(average_trials = ...(trials_domestic))\n\ntrials_region\n\nNow, let’s the number of transitions (dtr). Why are there so many transitions?\n\ntransitions_region = ...\n\ntransitions_region\n\nLet’s visualize various statistics! Combine trials_region and transitions_region using left_join(). Draw a scatteplot. Let transitions be on the X axis, and average_trials on the Y axis. Add geom_text() with label argument set to region, use vjust = 1.5.\n\ntrials_region %&gt;%\n  ...\n  ggplot(...(x = ..., y = average_trials)) +\n    geom_point(size = 4) +\n    geom_text(aes(label = region), vjust = 1.5) \n\nUsing ggpairs() visualize the following columns: \"trials_domestic\", \"region\", \"tcs_operated\", \"dtr\". What do you notice?\n\nlibrary(...)\n...\n\nSet up a multiple linear regerssion. Predict democratic transition dtr by trials_domestic and tcs_operated. What type of model did we create based on the independent variables (IVs)? Present the results.\n\nmodel = ...\nsummary(model)\n\nLoad ggeffects library\n\nlibrary(ggeffects)\n\nVisualize the model with terms trials_domestic and tcs_operated.\n\nggpredict(model, terms = c(\"trials_domestic\", \"tcs_operated\")) %&gt;%\n  plot()\n\nCheck the normality of residuals.\n\n...(model, which = 2)\n\nSet the linear probability model, but include country and year fixed effects.\n\nmodel_lpm = lm(dtr ~ trials_domestic + tcs_operated + ..., tjet_sub) \n\nCheck the normality, again. Use Q-Q plot.\n\nplot(model_lpm, 2)\n\nNow, let’s check if the residuals are homoscedastic. Use which = 1 for plot(). What’s that? Can we fix that? Or even, should we?\n\nplot(model_lpm, 1)\n\n\n\nLogistic Regression\nAs you can see, one of the assumptions is violated. You might try to solve it using SEs - but again, these are not panacea. A slightly more common approach to deal with binary dependent variable is logistic regression. Let’s briefly try it out!\n\nmodel_glm = glm(dtr ~ trials_domestic + tcs_operated, tjet_sub, family = binomial(link = \"logit\")) \nsummary(model_glm)\n\nNow, visualize how it looks like. Usually, we would expect to see slightly more “S” shaped curve. But still - good!\n\nggpredict(model_glm, term = c(\"trials_domestic\", \"tcs_operated\")) %&gt;%\n  plot(show_data = T)\n\nCompare to linear probability model without country and year fixed effects.\n\nggpredict(model, term = c(\"trials_domestic\", \"tcs_operated\")) %&gt;%\n  plot(show_data = T)\n\nPresent both: linear probability model without fixed effects and logistic regression. The problem, though, that Logit is way harder to interpret! Stick to marginal effects, which we have covered in lab 5!\n\nlibrary(modelsummary)\n\nmodelsummary(list(\"LPM\" = model,\n                  \"Logit\" = model_glm),\n             stars = T)\n\nA final note, in most cases you would expect to see logistic regression as something like\n\nglm(am ~ hp + wt, data = mtcars, family = binomial) %&gt;%\n  ggpredict(terms = c(\"hp\")) %&gt;%\n  plot(show_data = T)\n\n\n\n\n\n\n\n\n\n\nWhat’s next?\nCongrats with finalizing the class! No exercises this time.\n\nHelpful resource for R\nCheck the main page for additional resources\nLearning to code is like learning a new language: if you’re interested, keep practicing!\nI highly recommend taking POLI_SCI 406-0 Quantitative Causal Inference\n\n\n\nWhat we have covered in coding\n\n\n\n\n\n\n\n\nLibrary\nFunctions\nDescription\n\n\n\n\ntidyverse\nfilter(), mutate(), ggplot()\ndata wrangling and visualization\n\n\nmodelsummary\nmodelsummary()\npresent good looking tables\n\n\nbroom\ntidy()\nextract additional information from the models\n\n\nggeffects\nggpredict()\ncalculate and visualize marginal effects\n\n\nGGally\nggpairs()\nextension to ggplot\n\n\nlmtest\nbptest()\nadditional statistical tests for diagnostics\n\n\nestimatr\nlm_robust()\nregression with robust and clustered standard errors\n\n\ncar\nvif()\nadditional statistical tests for diagnostics\n\n\n\n\n\nDatasets we have used\n\n\n\nDataset\nDescription\nLink\n\n\n\n\nV-Dem\nMeasures democracy worldwide\nV-Dem\n\n\nWorld Happiness Report\nAnnual happiness report\nWorld Happiness Report\n\n\nWho Governs\nDataset on political elites\nWho Governs\n\n\nSIPRI\nData on military operations\nSIPRI\n\n\nComparative Political Data Set\nA dataset covering political institutions\nCPDS\n\n\nEuropean Social Survey\nSurvey measuring attitudes across Europe\nESS\n\n\nTransitional Justice Evaluation Tools Dataset\nDataset assessing transitional justice\nTJET Dataset\n\n\n\n\n\nCheck list\n I know that every lab has check list below! And I will use it to navigate what we have learned\n R doesn’t scare me anymore\n I have developed the intuition behind application of quantitative methods"
  },
  {
    "objectID": "ps405-d_8_extra.html",
    "href": "ps405-d_8_extra.html",
    "title": "Lab 8 Extra",
    "section": "",
    "text": "Introduction\nFirst of all, I’m glad you opened this link! Feel free to copy the code for your purposes.\nNot all data comes in a neat and clean way. Here I’ll show you how I prepared the data for Lab 8.\n\n\nTurning to Business\nAs usual, let’s load the library tidyverse for data wrangling and kableExtra for better display of the results in HTML document.\n\nlibrary(tidyverse)\nlibrary(kableExtra)\n\nNow, let’s load the raw dataset.\n\ndf = read.csv(\"data_raw/ESS11-subset.csv\")\n\nHere is how it looks like:\n\ndf %&gt;%\n  head() %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nessround\nedition\nproddate\nidno\ncntry\ndweight\npspwght\npweight\nanweight\npplfair\ntrstplt\ntrstprt\nhappy\nrlgdgr\nedulvlb\nprob\nstratum\npsu\n\n\n\n\nESS11e02\n11\n2\n20.11.2024\n50014\nAT\n1.1851145\n0.3928906\n0.3309145\n0.1300132\n5\n5\n5\n8\n5\n322\n0.0005786\n107\n317\n\n\nESS11e02\n11\n2\n20.11.2024\n50030\nAT\n0.6098981\n0.3251533\n0.3309145\n0.1075980\n0\n1\n0\n9\n0\n423\n0.0011244\n69\n128\n\n\nESS11e02\n11\n2\n20.11.2024\n50057\nAT\n1.3923296\n4.0000234\n0.3309145\n1.3236659\n9\n4\n4\n9\n8\n610\n0.0004925\n18\n418\n\n\nESS11e02\n11\n2\n20.11.2024\n50106\nAT\n0.5560615\n0.1762276\n0.3309145\n0.0583163\n6\n3\n3\n7\n6\n422\n0.0012333\n101\n295\n\n\nESS11e02\n11\n2\n20.11.2024\n50145\nAT\n0.7227953\n1.0609399\n0.3309145\n0.3510804\n3\n5\n5\n9\n1\n322\n0.0009488\n115\n344\n\n\nESS11e02\n11\n2\n20.11.2024\n50158\nAT\n0.9926053\n1.3928125\n0.3309145\n0.4609019\n8\n5\n5\n8\n3\n313\n0.0006909\n7\n373\n\n\n\n\n\nYou can notice a lot of variables there, and observations for one country repeated multiple times. As it’s a survey data, ESS asks a lot of people from each European Union country. In the Lab we use aggregated data for each country, thus we need to calculate the average.\nTake a look on their codebook here. Check how the “pplfair - Most people try to take advantage of you, or try to be fair” variable is coded:\n\n\n\nValue\nCategory\n\n\n\n\n0\nMost people try to take advantage of me\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\nMost people try to be fair\n\n\n77\nRefusal*\n\n\n88\nDon’t know*\n\n\n99\nNo answer*\n\n\n\nThus, we need to recode values 77, 88 and 99 to something else, otherwise our aggregated values can get over 10 when the scale doesn’t assume it. Let’s see:\n\ndf %&gt;%\n  ggplot(aes(x = pplfair)) +\n  geom_histogram() \n\n\n\n\n\n\n\n\nLet’s record the average without correcting the data.\n\navg_raw = mean(df$pplfair)\n\nYou can see how I recode the variables below using case_when() from tidyverse. I rely on documentation of the European Social Survey, as you should do too! This weird NA_real_ makes values simply NA. Syntax is not as straightforward, take some time to understand what’s going on!\n\ndf = df %&gt;%\n  mutate(pplfair = case_when(pplfair == 99 ~ NA_real_,\n                             pplfair == 88 ~ NA_real_,\n                             pplfair == 77 ~ NA_real_,\n                             TRUE ~ pplfair),\n         trstplt = case_when(trstplt == 99 ~ NA_real_,\n                             trstplt == 88 ~ NA_real_,\n                             trstplt == 77 ~ NA_real_,\n                             TRUE ~ trstplt),\n         trstprt = case_when(trstprt == 99 ~ NA_real_,\n                             trstprt == 88 ~ NA_real_,\n                             trstprt == 77 ~ NA_real_,\n                             TRUE ~ trstprt),\n         rlgdgr = case_when(rlgdgr == 99 ~ NA_real_,\n                             rlgdgr == 88 ~ NA_real_,\n                             rlgdgr == 77 ~ NA_real_,\n                             TRUE ~ rlgdgr),\n         happy = case_when(happy == 99 ~ NA_real_,\n                             happy == 88 ~ NA_real_,\n                             happy == 77 ~ NA_real_,\n                             TRUE ~ happy),\n         edulvlb = case_when(edulvlb &lt; 610 ~ 1,\n                             edulvlb == 610 | edulvlb == 620 ~ 2,\n                             edulvlb == 710 | edulvlb == 720 ~ 3,\n                             edulvlb == 800 ~ 4,\n                             TRUE ~ NA_real_))\n\nNow, let’s calculate average. Don’t forget, now we have NAs, so we need to use na.rm = TRUE to avoid errors.\n\navg_corr = mean(df$pplfair, na.rm = TRUE)\n\nNow, compare. This might be quite meaningful!\n\ndata.frame(`Raw` = avg_raw, `Recoded` = avg_corr) %&gt;%\n  kable()\n\n\n\n\nRaw\nRecoded\n\n\n\n\n6.131985\n5.708889\n\n\n\n\n\nNow, let’s group the data by country to aggregate variables to the country level.\n\ndf_groupped = df %&gt;%\n  group_by(cntry) %&gt;%\n  summarize(pplfair = mean(pplfair, na.rm = T),\n            trstplt = mean(trstplt, na.rm = T),\n            trstprt = mean(trstprt, na.rm = T),\n            rlgdgr = mean(rlgdgr, na.rm = T),\n            happy = mean(happy, na.rm = T),\n            edulvlb = mean(edulvlb, na.rm = T))\n\nhead(df_groupped) %&gt;%\n  kable()\n\n\n\n\ncntry\npplfair\ntrstplt\ntrstprt\nrlgdgr\nhappy\nedulvlb\n\n\n\n\nAT\n6.385696\n3.773544\n3.723961\n4.612917\n7.781570\n1.243568\n\n\nBE\n6.076585\n4.172808\n3.942565\n4.145283\n7.778544\n1.642675\n\n\nCH\n6.431571\n5.540824\n5.292646\n4.482909\n8.154348\n1.498912\n\n\nCY\n4.428363\n2.681481\n2.497041\n6.756598\n6.969118\n1.469173\n\n\nDE\n6.177006\n4.004165\n3.974069\n3.922599\n7.762929\n1.435993\n\n\nES\n5.578890\n2.784035\n2.776864\n4.104518\n7.847991\n1.438520\n\n\n\n\n\nFinally, save the data. This is the data we have been working in the Lab 8.\n\nwrite.csv(df_groupped, \"data/ess.csv\", row.names = F)"
  }
]