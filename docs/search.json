[
  {
    "objectID": "ps405-d_1.html",
    "href": "ps405-d_1.html",
    "title": "Last Quarter’s Review",
    "section": "",
    "text": "We are expected to have installed R and RStudio, if not see the installing R section.\nIn the discussion section, we will focus on coding and practicing what we have learned in the lectures.\nOffice hours are on Tuesday, 11-12:30 Scott 110.\nQuestions?\n\n\nDownload script"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PS405 Linear Models",
    "section": "",
    "text": "This site is designed to support students enrolled in the PS405 Linear Models course. Here, you will find lab materials, which are also available on Canvas. In the section, we will focus on coding and practicing the concepts covered in the lectures.\n\n\n\n\n\n   Thursday\n   11:00 AM – 11:50 AM\n   Scott Hall 212\n\n\n\n\n\n\n   Tuesday\n   11:00 AM – 12:30 PM\n   Scott Hall 110"
  },
  {
    "objectID": "ps405-d_1.html#terminology",
    "href": "ps405-d_1.html#terminology",
    "title": "Last Quarter’s Review",
    "section": "Terminology",
    "text": "Terminology"
  },
  {
    "objectID": "ps405-d_1.html#coding-terminology",
    "href": "ps405-d_1.html#coding-terminology",
    "title": "Last Quarter’s Review",
    "section": "Coding Terminology",
    "text": "Coding Terminology\n\nCode Chunk\nTo insert a Code Chunk, you can use Ctrl+Alt+I on Windows and Cmd+Option+I on Mac. Run the whole chunk by clicking the green triangle, or one/multiple lines by using Ctrl + Enter or Command + Return on Mac.\n\nprint(\"Code Chunk\")\n\n[1] \"Code Chunk\"\n\n\n\n\nFunction and Arguments\nMost of the functions we want to run require an argument For example, the function print() above takes the argument “Code Chunk”.\n\nfunction(argument)\n\n\n\nData structures\nThere are many data structures, but the most important to know the following.\n\nObjects. Those are individual units, e.g. a number or a word.\n\n\nnumber = 1\nnumber\n\nword = \"Northwestern\"\nword\n\n[1] 1\n[1] \"Northwestern\"\n\n\n\nVectors. Vectors are collections of objects. To create one, you will need to use function c().\n\n\nnumbers = c(1, 2, 3)\nnumbers\n\n[1] 1 2 3\n\n\n\nDataframes. Dataframes are the most used data structure. Last quarter you spend a lot of time working with it. It is a table with data. Columns are called variables, and those are vectors. You can access a column using $ operator.\n\n\ndf = data.frame(numbers, \n                numbers_multiplied = numbers * 2)\ndf\ndf$numbers_multiplied\n\n  numbers numbers_multiplied\n1       1                  2\n2       2                  4\n3       3                  6\n[1] 2 4 6\n\n\n\n\nData classes\nWe work with various classes of data, and the analysis we perform depends heavily on these classes.\n\nNumeric. Continuous data.\n\n\nnumeric_class = c(1.2, 2.5, 7.3)\nnumeric_class\nclass(numeric_class)\n\n[1] 1.2 2.5 7.3\n[1] \"numeric\"\n\n\n\nInteger. Whole numbers (e.g., count data).\n\n\ninteger_class = c(1:3)\nclass(integer_class)\n\n[1] \"integer\"\n\n\n\nCharacter. Usually, represent textual data.\n\n\nword\n\n[1] \"Northwestern\"\n\nclass(word)\n\n[1] \"character\"\n\n\n\nFactor. Categorical variables, where each value is treated as an identifier for a category.\n\n\ncolors = c(\"blue\", \"green\")\nclass(colors)\n\n[1] \"character\"\n\n\nAs you noticed, R did not identify the class of data correctly. We can change it using as.factor() function. You can easily change the class of your variable (as.numeric(), as.integer(), as.character())\n\ncolors = as.factor(colors)\nclass(colors)\n\n[1] \"factor\"\n\n\n\n\nLibraries\nQuite frequently, we use additional libraries to extend the capabilities of R. I’m sure you remember tidyverse. Let’s load it.\n\nlibrary(tidyverse)\n\nIf you updated your R or recently downloaded it, you can easily install libraries using the function install.packages().\n\n\nPipes\nPipes (%&gt;% or |&gt;) are helpful for streamlining the coding. They introduce linearity to the process of writing the code. In plain English, a pipe translates to “take an object, and then”.\n\nnumbers %&gt;%\n  print()\n\n[1] 1 2 3"
  },
  {
    "objectID": "ps405-d_2.html",
    "href": "ps405-d_2.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Download the data\nOrganize your directory\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_2.html#model-building",
    "href": "ps405-d_2.html#model-building",
    "title": "Simple Linear Regression",
    "section": "Model Building",
    "text": "Model Building\nLet’s run a simple model, and then check it’s summary.\n\nbasic_model = lm(Ladder_score ~ Social_support, whr)\n  \nsummary(basic_model)\n\n\nCall:\nlm(formula = Ladder_score ~ Social_support, data = whr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76562 -0.36701  0.01165  0.46577  1.49971 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.3428     0.3386  -1.013    0.313    \nSocial_support   7.3618     0.4183  17.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6304 on 135 degrees of freedom\nMultiple R-squared:  0.6964,    Adjusted R-squared:  0.6942 \nF-statistic: 309.7 on 1 and 135 DF,  p-value: &lt; 2.2e-16\n\n\nA one unit increase in Social Support is associated with a 7.4 increase in the happiness score. What is the maximum value the Happiness Score can take?\n\nmax(whr$Ladder_score)\n\n[1] 7.804\n\n\nAnd now, let’s draw a histogram of the Social Support. So, how much does this model tell us?\n\nggplot(whr) +\n  ...(aes(x = Social_support))\n\nLet’s correct the Social_support variable a bit, transforming it to 0-100 scale. What do you think about the model now? What do you think about \\(R^2\\)?\n\nwhr = whr %&gt;%\n  mutate(Social_support_percentage = Social_support * 100)\n\nadjusted_model = lm(Ladder_score ~ Social_support_percentage, whr)\n  \nsummary(adjusted_model)\n\n\nCall:\nlm(formula = Ladder_score ~ Social_support_percentage, data = whr)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76562 -0.36701  0.01165  0.46577  1.49971 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -0.342811   0.338568  -1.013    0.313    \nSocial_support_percentage  0.073618   0.004183  17.599   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6304 on 135 degrees of freedom\nMultiple R-squared:  0.6964,    Adjusted R-squared:  0.6942 \nF-statistic: 309.7 on 1 and 135 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s write this regression formula out. Do you remember the general form?\n\\[\nY = \\beta_0 + \\beta_1X_1+\\epsilon\n\\]\nIn our case, this can be presented as\n\\[\n\\text{Happines} = -0.34 + 0.07\\text{ Social Support} + e\n\\]\nAlternatively,\n\\[\nY = -0.34+0.07x+u\n\\]\nNow, visualize the regression.\n\nggplot(whr, aes(x = Social_support_percentage, y = Ladder_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Social Support (%)\",\n       y = \"Happiness Score\")"
  },
  {
    "objectID": "ps405-d_2.html#diagnostics",
    "href": "ps405-d_2.html#diagnostics",
    "title": "Simple Linear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics\nLet’s analyze the regression. First, extract the residuals and plot their distribution. Does it follow \\(N(0, \\sigma^2)\\)?\n\nres = adjusted_model$residuals\n\nggplot() +\n  geom_histogram(aes(x = res), bins = 20) +\n  geom_vline(xintercept = mean(res), color = \"red\", size = 1.5)\n\n\n\n\n\n\n\n\nNow we need to check the constant variance assumption. Does it hold? What term is used to describe this satisfied assumption?\n\nyhat = adjusted_model$fitted.values\n\nggplot() +\n  geom_point(aes(x = yhat, y = res)) +\n  geom_hline(yintercept = 0, color = \"blue\") +\n  labs(title = \"Residuals vs fitted values plot\")\n\n\n\n\n\n\n\n\nExplore different patterns below"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "PS405 Linear Models",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nLink\n\n\nCode\n\n\nData\n\n\n\n\n\n\n1\n\n\nLast Quarter’s Review\n\n\n\n\n\n\n\n\n \n\n\n\n\n2\n\n\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\nReview and Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "PS405 Linear Models",
    "section": "",
    "text": "This site is designed to support students enrolled in the PS405 Linear Models course. Here, you will find lab materials, which are also available on Canvas. In the section, we will focus on coding and practicing the concepts covered in the lectures.\n\n\n\n\n\n   Thursday\n   11:00 AM – 11:50 AM\n   Scott Hall 212\n\n\n\n\n\n\n   Tuesday\n   11:00 AM – 12:30 PM\n   Scott Hall 110"
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "PS405 Linear Models",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nHelpful Regression Resources\n\nStep-by-step regression guide\n\n\n\nData sources\n\nPolitical Science datasets\n\n\n\nVisualization\n\nIntroduction to ggplot2\nMaps/Networks/Advanced vizualisation with ggplot2\nMaking interactive plots with ggplot2"
  },
  {
    "objectID": "ps405-d_3.html",
    "href": "ps405-d_3.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Congrats with submitting the first HW! How are you feeling?\nThe discussion section structure (review, comments about HW, new material and exercises). I would be happy to hear your feedback after the classes or via email.\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_3.html#fixed-effects",
    "href": "ps405-d_3.html#fixed-effects",
    "title": "Multiple Linear Regression",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nLet’s explore how leader’s tenure is associated with the number of individuals in the government. We start with the simple linear regression. Take a moment to interpret the result and \\(R^2\\).\n\nlm(n_individuals ~ leaderexperience_continuous, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.666  -6.937  -1.937   5.301 109.063 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 23.59948    0.16232  145.39   &lt;2e-16 ***\nleaderexperience_continuous  0.33702    0.01627   20.71   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.21 on 9151 degrees of freedom\nMultiple R-squared:  0.04477,   Adjusted R-squared:  0.04467 \nF-statistic: 428.9 on 1 and 9151 DF,  p-value: &lt; 2.2e-16\n\n\nTake a moment and draw a scatterplot for n_individuals and leaderexperience_continuous. Add a regression line to the plot.\n\n...\n\nNow, let’s add a categorical variable, indep, to the model. By doing so, we assume that the association between the leader’s tenure and the number of individuals in the government differs depending on whether the leader is independent or partisan.\nPractically, this could be done in multiple ways. First, let’s discuss introduction of fixed effects to our model. Moreover, this is a Multiple linear regression!\n\nmodel_fe = lm(n_individuals ~ leaderexperience_continuous + indep, whogov) \nsummary(model_fe)\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous + indep, \n    data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.437  -7.078  -1.796   5.377 108.632 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 24.01096    0.16605  144.60   &lt;2e-16 ***\nleaderexperience_continuous  0.35700    0.01617   22.08   &lt;2e-16 ***\nindepIndependent            -3.02909    0.29988  -10.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 9126 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.05649,   Adjusted R-squared:  0.05628 \nF-statistic: 273.2 on 2 and 9126 DF,  p-value: &lt; 2.2e-16\n\n\nWe will use ggeffects library for visualization of regression with the fixed effects. This is sort of an addition to ggplot2 library from tidyverse. Don’t forget to install it using install.packages()!\n\nlibrary(ggeffects)\n\nThen, visualize the result. What can we see?\n\nggpredict(model_fe, terms = c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nLet’s customize the plot. It should be relatively straightforward given we know ggplot functions. Details for the customization of plot() function can be found on ggeffects website.\n\nggpredict(model_fe, terms= c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot(show_ci = F) +\n  labs(title = \"Fixed Effects Regression\",\n       x = \"Tenure of a Leader\",\n       y = \"Number of Individuals in a Cabinet\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nSome common fixed effects include:\n\nCountry/Region/State\nIndividual leaders/Parties\nYear/Time\nPolicy presence or absence\n\nBy introducing fixed effects, we are able to control for unobserved confounders that vary across the units (not within!)."
  },
  {
    "objectID": "ps405-d_3.html#interactions",
    "href": "ps405-d_3.html#interactions",
    "title": "Multiple Linear Regression",
    "section": "Interactions",
    "text": "Interactions\nOften dummy variables are used to introduce an interaction term in the model. We will explore the association between Perceptions_of_corruption and number of people in the cabinet (n_individuals) depending on the independence of the party leader.\nThe task isn’t trivial as now we planning to use data from two datasets, Let’s subset those.\n\nwhr_subset = whr %&gt;%\n  select(Country_name, Perceptions_of_corruption)\n\nwhogov_subset = whogov %&gt;%\n  filter(year == 2021) %&gt;%\n  select(country_name, n_individuals, indep)\n\nNow, we are merging them. It’s not easy to understand what’s going on, but here is a great resource on joins (check “the {dplyr} way”)!\n\nwhr_whogov = whr_subset %&gt;%\n  left_join(whogov_subset, by = c(\"Country_name\" = \"country_name\")) \n\nCheck the result\n\nhead(whr_whogov)\n\n  Country_name Perceptions_of_corruption n_individuals           indep\n1      Finland                     0.182            23 Non-independent\n2      Denmark                     0.196            24 Non-independent\n3      Iceland                     0.668            15 Non-independent\n4       Israel                     0.708            33 Non-independent\n5  Netherlands                     0.379            18 Non-independent\n6       Sweden                     0.202            26 Non-independent\n\n\nNow, to interact variables we need to use asterisk *, i.e. multiplication.\n\nmodel_in = lm(Perceptions_of_corruption ~ n_individuals * indep, whr_whogov)\nsummary(model_in)\n\n\nCall:\nlm(formula = Perceptions_of_corruption ~ n_individuals * indep, \n    data = whr_whogov)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54000 -0.05737  0.04300  0.10864  0.24382 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     0.575192   0.061646   9.331 5.59e-16 ***\nn_individuals                   0.004818   0.002151   2.240   0.0269 *  \nindepIndependent                0.386220   0.159570   2.420   0.0170 *  \nn_individuals:indepIndependent -0.010703   0.005501  -1.946   0.0540 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1737 on 123 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.08324,   Adjusted R-squared:  0.06088 \nF-statistic: 3.723 on 3 and 123 DF,  p-value: 0.01326\n\n\nLet’s plot the result. Try to change show_ci to TRUE. Does it explain the p-value now?\n\nggpredict(model_in, terms= c(\"n_individuals\", \"indep\")) %&gt;%\n  plot(show_ci = FALSE) +\n  labs(title = \"Regression with Interaction Term\",\n       x = \"Number of Individuals in a Cabinet\",\n       y = \"Perception of Corruption\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAnd you can easily simulate the data (i.e., calculate the marginal effect) using ggpredict(). For example,\n\nggpredict(model_in, terms= c(\"n_individuals [12]\", \"indep [Independent]\"))\n\n# Predicted values of Perceptions_of_corruption\n\nn_individuals | Predicted |     95% CI\n--------------------------------------\n           12 |      0.89 | 0.71, 1.07"
  },
  {
    "objectID": "ps405-d_3.html#using-dummy-variables-in-the-regression",
    "href": "ps405-d_3.html#using-dummy-variables-in-the-regression",
    "title": "Multiple Linear Regression",
    "section": "Using dummy variables in the regression",
    "text": "Using dummy variables in the regression\nToday we are working with WhoGov dataset. As usual, I recomment taking a look at their codebook.\n\nwhogov = read.csv(\"data/WhoGov.csv\")\n\nFirst of all, these are the following variables we are going to work with today:\n\ncountry_name is a country name\nn_individuals number of unique persons in the cabinet\nleaderexperience_continuous the number of years the person has been leader of the country in total.\nleader_party party of the leader\nsystem_category the regime type\n\nStart with exploring the distribution of number of unique persons in the cabinet (n_individuals)\n\nggplot(whogov) +\n  geom_histogram(aes(x = n_individuals)) \n\n\n\n\n\n\n\n\nPresent the descriptive statistics of n_individuals variable.\n\n...\n\nLet’s explore whether a leader of a country being independent from a party leads to more or fewer people in their cabinet. First, let’s create a dummy variable indicating if a leader is independent or non-independent. You can use 1 or 0 instead, but to make it more readable we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Non-independent\"))\n\nNow, build a simple model and explore the effect.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           24.0592     0.2761  87.148   &lt;2e-16 ***\nindepNon-independent   2.2833     0.3058   7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 пропущенных наблюдений удалены)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14\n\n\nWhat if we want to relevel the indep variable to know the effect relative to Non-independent leader?\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nOops! This is why classes of data are important. Fix it!\n\nwhogov$indep = as.factor(whogov$indep)\n\nNow we can relevel the variable\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nCompare the models. Does the result sound reasonable? Pretty much. This is simply an inverse. But things get way more interesting if categorical variable has more than 2 levels. You will see this later on. For now, let’s interpret the result.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       26.3425     0.1315 200.264   &lt;2e-16 ***\nindepIndependent  -2.2833     0.3058  -7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 пропущенных наблюдений удалены)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14"
  },
  {
    "objectID": "ps405-d_3.html#dummy-variables-and-ols-regression",
    "href": "ps405-d_3.html#dummy-variables-and-ols-regression",
    "title": "Multiple Linear Regression",
    "section": "Dummy variables and OLS regression",
    "text": "Dummy variables and OLS regression\nToday we are working with WhoGov dataset. As usual, I recomment taking a look at their codebook.\n\nwhogov = read.csv(\"data/WhoGov.csv\")\n\nFirst of all, these are the following variables we are going to work with today:\n\ncountry_name is a country name\nn_individuals number of unique persons in the cabinet\nleaderexperience_continuous the number of years the person has been leader of the country in total.\nleader_party party of the leader\n\nStart with exploring the distribution of number of unique persons in the cabinet (n_individuals)\n\nggplot(whogov) +\n  geom_histogram(aes(x = n_individuals)) \n\n\n\n\n\n\n\n\nPresent the descriptive statistics of n_individuals variable.\n\n...\n\nLet’s examine whether a country’s leader being independent from a political party is associated with having more or fewer members in their cabinet. First, let’s create a dummy variable indicating if a leader is independent or non-independent. You can use 1 or 0 instead, but to make it more readable here we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Non-independent\"))\n\nNow, build a simple model and explore the effect. On average, being a non-independent (i.e. partisan) leader is associated with having 2.28 more members in their cabinet compared to independent leaders.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           24.0592     0.2761  87.148   &lt;2e-16 ***\nindepNon-independent   2.2833     0.3058   7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14\n\n\nWhat if we want to know the effect relative to Non-independent leader? Let’s relevel() the variable!\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nOops! This is why classes of data are important. Fix it!\n\nwhogov$indep = as.factor(whogov$indep)\n\nNow we can relevel the variable\n\nwhogov$indep = relevel(whogov$indep, ref = \"Non-independent\")\n\nCompare the models. Does the result sound reasonable? Pretty much. This is simply an inverse. But things get way more interesting if a categorical variable has more than 2 levels. You will see this later on.\n\nlm(n_individuals ~ indep, whogov) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = n_individuals ~ indep, data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.342  -7.342  -2.342   5.658 106.658 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       26.3425     0.1315 200.264   &lt;2e-16 ***\nindepIndependent  -2.2833     0.3058  -7.466    9e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.35 on 9127 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.006071,  Adjusted R-squared:  0.005962 \nF-statistic: 55.75 on 1 and 9127 DF,  p-value: 9.003e-14"
  },
  {
    "objectID": "index.html#grade-increase-policy",
    "href": "index.html#grade-increase-policy",
    "title": "PS405 Linear Models",
    "section": "Grade increase policy",
    "text": "Grade increase policy\nTo improve your homework grade, you should submit the exercises from the corresponding lab session held during the week the homework was assigned. For instance, if you wish to improve the grade for the first homework, which was assigned on January 16th, you should submit the completed exercises from the second lab, which took place on the same day. If you prefer not to check the dates each time, given that homework is assigned each week starting from week 2, you can use the formula:\n\\[\n\\text{E}[\\text{Lab Number}|\\text{Pset is assigned each week}] = \\text{Pset Number} + 1\n\\]"
  },
  {
    "objectID": "index.html#pset-grade-increase-policy",
    "href": "index.html#pset-grade-increase-policy",
    "title": "PS405 Linear Models",
    "section": "Pset Grade Increase Policy",
    "text": "Pset Grade Increase Policy\nTo improve your grade for the problem set, you should submit the exercises from the corresponding lab session held during the week the homework was assigned. Exercises can be found at the end of each lab. You can increase grades for every problem set, except for the last one before the final. The maximum possible increase is two points.\nFor instance, if you wish to improve the grade for the first homework, which was assigned on January 16th, you should submit the completed exercises from the second lab, which took place on the same day. If you prefer not to check the dates each time, given that homework is assigned each week starting from week 2, you can use the formula:\n\\[\n\\text{Lab Number}|\\text{Pset is assigned each week} = \\text{Pset Number} + 1\n\\]\nAfter the grades are released, you have one week to submit the .qmd file via email to artur.baranov@u.northwestern.edu."
  },
  {
    "objectID": "ps405-d_4.html",
    "href": "ps405-d_4.html",
    "title": "Review and Confidence Intervals",
    "section": "",
    "text": "Congrats passing through the first quiz! How are you feeling?\nGrades for the psets are out.\n\n\nDownload script\n\n\n\nDownload data"
  },
  {
    "objectID": "ps405-d_4.html#latex-issues",
    "href": "ps405-d_4.html#latex-issues",
    "title": "Review and Confidence Intervals",
    "section": "\\(\\LaTeX\\) issues",
    "text": "\\(\\LaTeX\\) issues\nIntegration of R, Python, markdown, Latex and other useful languages incredibly useful. But it comes to a price that researchers should be careful. Any \\(\\LaTeX\\) code should go within two $ dollar signs $. For example, an inline formula looks like this: $ Y = 10 $, which produces the following result: \\(Y = 10\\). Alternativelym you can use a double dollar sign to start a “chunk” for latex. For example:\n\\[\nY = \\beta_0 + \\beta_1 + u\n\\]"
  },
  {
    "objectID": "ps405-d_4.html#useful-functions",
    "href": "ps405-d_4.html#useful-functions",
    "title": "Review and Confidence Intervals",
    "section": "Useful functions",
    "text": "Useful functions\nSometimes you might want to visualize some mathematical functions using geom_function(). In the HW, you were asked to plot an OLS regression vs the true data generating process. For this purpose example below is super useful (however, geom_abline() for this particular task is perfect, too).\n\nset.seed(123)\n\nexample_data = data.frame(x = rnorm(100,\n                                    mean = 2,\n                                    sd = 15),\n                          y = rnorm(100,\n                                    mean = 2,\n                                    sd = 15) * rnorm(100))\n\nggplot(example_data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(aes(color = \"Fitted\"), method = \"lm\", se = F) +\n  geom_function(aes(color = \"Known\"), fun = function(x){1 + 0.5 * x}, size = 1.2) \n\n\n\n\n\n\n\n\nSometimes it’s useful to visualize, say, polynomials! How does \\(x^3\\) look like?\n\nggplot() +\n  geom_function(fun = function(x){x^2}) +\n  geom_vline(xintercept = 0) +\n  xlim(-1, 1)"
  },
  {
    "objectID": "ps405-d_4.html#fixed-effects",
    "href": "ps405-d_4.html#fixed-effects",
    "title": "Review and Confidence Intervals",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nWhen we introduce a dummy variable into the model, we quite often are interested in controlling for unobserved heterogeneity by allowing each category to have its own intercept. This is referred to as fixed effects.\nFirst, let’s create a dummy variable indicating if a leader is independent or Partisan You can use 1 or 0 instead, but to make it more readable here we stick to more transparent labels.\n\nwhogov = whogov %&gt;%\n  mutate(indep = ifelse(leader_party == \"independent\", \"Independent\", \"Partisan\"))\n\nDon’t forget about the class of the variable!\n\nwhogov$indep = as.factor(whogov$indep)\n\nTake a moment to think about what unobserved heterogeneity we can control for by including whether a leader is independent or partisan in the model.\n\nmodel_fe = lm(n_individuals ~ leaderexperience_continuous + indep, whogov) \nsummary(model_fe)\n\n\nCall:\nlm(formula = n_individuals ~ leaderexperience_continuous + indep, \n    data = whogov)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.437  -7.078  -1.796   5.377 108.632 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 20.98188    0.30295   69.26   &lt;2e-16 ***\nleaderexperience_continuous  0.35700    0.01617   22.08   &lt;2e-16 ***\nindepPartisan                3.02909    0.29988   10.10   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 9126 degrees of freedom\n  (24 observations deleted due to missingness)\nMultiple R-squared:  0.05649,   Adjusted R-squared:  0.05628 \nF-statistic: 273.2 on 2 and 9126 DF,  p-value: &lt; 2.2e-16\n\n\nTo understand what’s going on in the model we might want to visualize the result. Load the ggeffects library.\n\nlibrary(ggeffects)\n\nThen, visualize the result. What can we see?\n\nggpredict(model_fe, terms = c(\"leaderexperience_continuous\", \"indep\")) %&gt;%\n  plot() +\n  labs(title = \"Fixed Effects Regression\",\n       x = \"Tenure of a Leader\",\n       y = \"Number of Individuals in a Cabinet\",\n       color = \"Leader's Status\") +\n  theme_bw()"
  },
  {
    "objectID": "ps405-d_4.html#interactions",
    "href": "ps405-d_4.html#interactions",
    "title": "Review and Confidence Intervals",
    "section": "Interactions",
    "text": "Interactions\nOften dummy variables are used to introduce an interaction term in the model. We will explore the association between Perceptions_of_corruption and number of people in the cabinet (n_individuals) depending on the independence of the party leader.\nThe task isn’t trivial as now we planning to use data from two datasets, Let’s subset those.\n\nwhr_subset = whr %&gt;%\n  select(Country_name, Perceptions_of_corruption)\n\nwhogov_subset = whogov %&gt;%\n  filter(year == 2021) %&gt;%\n  select(country_name, n_individuals, indep)\n\nNow, we are merging them. Once again, there is a great resource for joins (check “the {dplyr} way”)!\n\nwhr_whogov = whr_subset %&gt;%\n  left_join(whogov_subset, by = c(\"Country_name\" = \"country_name\")) \n\nCheck the result of the left_join()\n\nhead(whr_whogov)\n\n  Country_name Perceptions_of_corruption n_individuals    indep\n1      Finland                     0.182            23 Partisan\n2      Denmark                     0.196            24 Partisan\n3      Iceland                     0.668            15 Partisan\n4       Israel                     0.708            33 Partisan\n5  Netherlands                     0.379            18 Partisan\n6       Sweden                     0.202            26 Partisan\n\n\nNow, let’s interact the variable\n\nmodel_in = lm(Perceptions_of_corruption ~ n_individuals * indep, whr_whogov)\nsummary(model_in)\n\n\nCall:\nlm(formula = Perceptions_of_corruption ~ n_individuals * indep, \n    data = whr_whogov)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.54000 -0.05737  0.04300  0.10864  0.24382 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  0.961412   0.147181   6.532 1.54e-09 ***\nn_individuals               -0.005885   0.005063  -1.162    0.247    \nindepPartisan               -0.386220   0.159570  -2.420    0.017 *  \nn_individuals:indepPartisan  0.010703   0.005501   1.946    0.054 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1737 on 123 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.08324,   Adjusted R-squared:  0.06088 \nF-statistic: 3.723 on 3 and 123 DF,  p-value: 0.01326\n\n\nLet’s plot the result.\n\nggpredict(model_in, terms= c(\"n_individuals\", \"indep\")) %&gt;%\n  plot(show_ci = FALSE) +\n  labs(title = \"Regression with Interaction Term\",\n       x = \"Number of Individuals in a Cabinet\",\n       y = \"Perception of Corruption\",\n       color = \"Leader's Status\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nI guess after solving pset and quiz you realize why interpreting interactions is hard? You can easily simulate the data (i.e., calculate the marginal effect) using ggpredict(). For example,\n\nggpredict(model_in, terms= c(\"n_individuals [12]\", \"indep [Independent]\"))\n\n# Predicted values of Perceptions_of_corruption\n\nn_individuals | Predicted |     95% CI\n--------------------------------------\n           12 |      0.89 | 0.71, 1.07"
  },
  {
    "objectID": "ps405-d_4.html#transformations",
    "href": "ps405-d_4.html#transformations",
    "title": "Confidence Intervals and Hypothesis Testing",
    "section": "Transformations",
    "text": "Transformations"
  },
  {
    "objectID": "ps405-d_4.html#iv-transformations",
    "href": "ps405-d_4.html#iv-transformations",
    "title": "Review and Confidence Intervals",
    "section": "IV Transformations",
    "text": "IV Transformations\nOne of the assumptions for OLS is that there is a linear relationship between the dependent and independent variables. However, this quite often not the case. Let’s reverse the correction made in World Happiness report data. See below. Does it look linear?\n\nggplot(whr) +\n  geom_point(aes(x = exp(Logged_GDP_per_capita), y = Ladder_score)) +\n  labs(x = \"GDP per capita\")\n\n\n\n\n\n\n\n\nIt doesn’t. It’s hard to describe this relationship in a linear manner. Natural log would explain this better, right?\n\nggplot(whr) +\n  geom_point(aes(x = exp(Logged_GDP_per_capita), y = Ladder_score)) +\n  geom_function(fun = function(x){log(x) - 4}) +\n  labs(x = \"GDP per capita\")\n\n\n\n\n\n\n\n\nThis is why we use the natural logarithm to transform GDP per capita. The transformation reveals a linear relationship between the two variables, allowing us to capture non-linear patterns in a linear format when using OLS regression. Another commonly used transformation is quadratic (\\(x^2\\)), which serves the same purpose of addressing non-linear relationships. We call latter ones polynomials.\n\nggplot(whr, aes(x = Logged_GDP_per_capita, y = Ladder_score)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "data_process.html",
    "href": "data_process.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nsipri = read.csv(\"import-export-values_1950-2024.csv\")\nsipri = sipri %&gt;%\n  select(-X2024)\n\n\nsipri = sipri %&gt;%\n  pivot_longer(2:75,\n               names_to = \"Year\",\n               values_to = \"Import\")\n\nsipri = sipri %&gt;%\n  mutate(Year = str_remove(Year, \"X\"))\n\nsipri = na.omit(sipri)\n\n\nload(url(\"https://github.com/vdeminstitute/vdemdata/raw/6bee8e170578fe8ccdc1414ae239c5e870996bc0/data/vdem.RData\"))\n\ne_v2x_polyarchy_5C\n\nvdem_sub = vdem %&gt;%\n  select(year, country_name, e_v2x_polyarchy_4C)\n\nvdem_sub$year = as.character(vdem_sub$year)\n\nmerger = sipri %&gt;%\n  left_join(vdem_sub, by = c('Year' = 'year', 'Recipient' = 'country_name'))\n\nmerger = na.omit(merger)\n\nmerger = merger %&gt;%\n  mutate(Regime = case_when(e_v2x_polyarchy_4C == 0 ~ \"Autocratic\",\n                            e_v2x_polyarchy_4C == 0.333 ~ \"Electoral Authoritarian\",\n                            e_v2x_polyarchy_4C == 0.667 ~ \"Minimally Democratic\",\n                            e_v2x_polyarchy_4C == 1 ~ \"Democratic\"))\n\nmerger = merger %&gt;%\n  select(-e_v2x_polyarchy_4C)\n\n# write.csv(merger, \"sipri.csv\", row.names = F)"
  }
]